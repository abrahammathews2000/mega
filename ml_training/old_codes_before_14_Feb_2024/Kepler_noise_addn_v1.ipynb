{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add noise to the simulated light curves\n",
    "# By add noise from Kepler Dataset\n",
    "# Here entire std dev is considered\n",
    "# Kepler_noise_addn_v1.ipynb\n",
    "\n",
    "\n",
    "# In other versions more refined std_dev will be considered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TF and check for GPU\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Import required libraries\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from tensorflow.keras.models import save_model, load_model\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "import random \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load Dataset\n",
    "## Load Train Set\n",
    "train_shape_dir = '/home/abraham/Documents/ms_proj_shape_lc_gen/data_npy/shape_npy/shape_filled5.npy'\n",
    "train_lc_dir = '/home/abraham/Documents/ms_proj_shape_lc_gen/data_npy/lc_npy/lc_dict_5.npy'\n",
    "train_lc = np.load(train_lc_dir)\n",
    "train_shape = np.load(train_shape_dir)\n",
    "# Check equality of number of dataset\n",
    "if len(train_lc)==len(train_shape):\n",
    "    print(\"Train Set: No. of LC = No. of shapes\")\n",
    "else:\n",
    "    sys.exit(\"EXIT: Train Set: No. of LC != No. of shapes\")\n",
    "\n",
    "## Load Validation Set\n",
    "vald_shape_dir = '/home/abraham/Documents/ms_proj_shape_lc_gen/data_npy/shape_npy/shape_filled4.npy'\n",
    "vald_lc_dir = '/home/abraham/Documents/ms_proj_shape_lc_gen/data_npy/lc_npy/lc_dict_4.npy'\n",
    "vald_lc = np.load(vald_lc_dir)\n",
    "vald_shape = np.load(vald_shape_dir)\n",
    "# Check equality of number of dataset\n",
    "if len(vald_lc)==len(vald_shape):\n",
    "    print(\"Vald Set: No. of LC = No. of shapes\")\n",
    "else:\n",
    "    sys.exit(\"Vald Set: No. of LC = No. of shapes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Normalize the image, convert to opacity map\n",
    "## Train Set\n",
    "train_shape = train_shape/np.amax(train_shape)\n",
    "train_shape_where_0 = np.where(train_shape == 0)\n",
    "train_shape_where_1 = np.where(train_shape == 1)\n",
    "train_shape[train_shape_where_0] = 1  # 1 represent the shape (1 opacity)\n",
    "train_shape[train_shape_where_1] = 0  # 0 represent background (0 opacity)\n",
    "\n",
    "## Valdn Set\n",
    "vald_shape = vald_shape/np.amax(vald_shape)\n",
    "vald_shape_where_0 = np.where(vald_shape == 0)\n",
    "vald_shape_where_1 = np.where(vald_shape == 1)\n",
    "vald_shape[vald_shape_where_0] = 1  # 1 represent the shape (1 opacity)\n",
    "vald_shape[vald_shape_where_1] = 0  # 0 represent background (0 opacity)\n",
    "print(\"Normalized the shape\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Normalize the lightcurves\n",
    "## - Train Set\n",
    "train_lc_scaled = np.zeros(train_lc.shape)\n",
    "for i in np.arange(len(train_lc_scaled)):\n",
    "    train_lc_scaled[i] = (train_lc[i] - np.amin(train_lc[i]))/(np.amax(train_lc[i]) - np.amin(train_lc[i]))\n",
    "\n",
    "## - Vald Set\n",
    "vald_lc_scaled = np.zeros(vald_lc.shape)\n",
    "for i in np.arange(len(vald_lc_scaled)):\n",
    "    vald_lc_scaled[i] = (vald_lc[i] - np.amin(vald_lc[i]))/(np.amax(vald_lc[i]) - np.amin(vald_lc[i]))\n",
    "print(\"Normalized the light curves\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add flat line towards left and right of dip\n",
    "# 50 data points on each side\n",
    "# 3. Extend the lightcurves\n",
    "## - Train Set\n",
    "train_lc_scaled_append = np.ones((train_lc.shape[0],120))\n",
    "print('train_lc_scaled_append.shape = ',train_lc_scaled_append.shape)\n",
    "print(\"len(train_lc_scaled_append[0,10:110]) = \",len(train_lc_scaled_append[0,10:110]))\n",
    "\n",
    "for i in np.arange(len(train_lc_scaled)):\n",
    "    train_lc_scaled_append[i,10:110] = train_lc_scaled[i]\n",
    "\n",
    "## - Vald Set\n",
    "vald_lc_scaled_append = np.ones((vald_lc.shape[0],120))\n",
    "for i in np.arange(len(vald_lc_scaled)):\n",
    "    vald_lc_scaled_append[i,10:110] = vald_lc_scaled[i]\n",
    "print(\"Extended the light curves\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't run above code more than once anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot - Train LCs\n",
    "num = 5\n",
    "fig,ax=plt.subplots(num,2, figsize=(5,8), gridspec_kw={ 'width_ratios': [2,1],\n",
    "        'wspace': 0.2,'hspace': 0.4})\n",
    "\n",
    "ax[0][1].set_title('Shape',size=15)\n",
    "ax[0][0].set_title('Light Curve (Train Dataset)',size=15)\n",
    "ax[num-1][0].set_xlabel('Phase',size=13)\n",
    "ph = np.linspace(-1,1,len(train_lc_scaled_append[0]))\n",
    "# advance = 60\n",
    "\n",
    "i = 0\n",
    "for i in np.arange(0,num):\n",
    "    k = random.randint(0, len(train_lc_scaled_append)-1)\n",
    "    ax[i][1].tick_params(left = False, right = False , labelleft = False ,labelbottom = False, bottom = False)\n",
    "    if(i<num-1): ax[i][0].tick_params(labelbottom = False, bottom = False)\n",
    "    img = ax[i][1].imshow(train_shape[k],cmap='inferno')\n",
    "    plt.colorbar(img)\n",
    "    ax[i][0].set_ylabel('Flux',size=13)\n",
    "    ax[i][0].set_ylim(-0.5,1.5)\n",
    "#     ax[i][0].scatter(ph, vald_lc_scaled_append[k],color = 'black',marker='.')\n",
    "    ax[i][0].plot(ph, train_lc_scaled_append[k],color = 'black',linewidth='3')\n",
    "    ax[i][0].grid('on')\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = ['/home/abraham/Documents/tfr_shallue/tfr_all/tfrecord/train-00000-of-00008',\n",
    "             '/home/abraham/Documents/tfr_shallue/tfr_all/tfrecord/train-00001-of-00008',\n",
    "             '/home/abraham/Documents/tfr_shallue/tfr_all/tfrecord/train-00002-of-00008',\n",
    "             '/home/abraham/Documents/tfr_shallue/tfr_all/tfrecord/train-00003-of-00008',\n",
    "             '/home/abraham/Documents/tfr_shallue/tfr_all/tfrecord/train-00004-of-00008',\n",
    "             '/home/abraham/Documents/tfr_shallue/tfr_all/tfrecord/train-00005-of-00008',\n",
    "             '/home/abraham/Documents/tfr_shallue/tfr_all/tfrecord/train-00006-of-00008',\n",
    "             '/home/abraham/Documents/tfr_shallue/tfr_all/tfrecord/train-00007-of-00008',\n",
    "             '/home/abraham/Documents/tfr_shallue/tfr_all/tfrecord/val-00000-of-00001',\n",
    "             '/home/abraham/Documents/tfr_shallue/tfr_all/tfrecord/test-00000-of-00001']\n",
    "raw_dataset = tf.data.TFRecordDataset(filenames)\n",
    "raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_description = {\n",
    "    'global_view': tf.io.FixedLenSequenceFeature([], tf.float32, default_value=0.0,allow_missing=True),\n",
    "    'local_view': tf.io.FixedLenSequenceFeature([], tf.float32, default_value=0.0,allow_missing=True),\n",
    "    'av_training_set': tf.io.FixedLenFeature([], tf.string, default_value=''),\n",
    "    'kepid': tf.io.FixedLenSequenceFeature([], tf.int64, default_value=int(0),allow_missing=True),\n",
    "}\n",
    "\n",
    "def _parse_function(example_proto):\n",
    "  # Parse the input `tf.train.Example` proto using the dictionary above.\n",
    "  return tf.io.parse_single_example(example_proto, feature_description)\n",
    "\n",
    "# At this point the dataset contains serialized tf.train.Example messages.\n",
    "# When iterated over it returns these as scalar string tensors.\n",
    "# Use the .take method to only show the first 10 records.\n",
    "for raw_record in raw_dataset.take(10):\n",
    "  print(repr(raw_record))\n",
    "  \n",
    "parsed_dataset = raw_dataset.map(_parse_function)\n",
    "parsed_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the size of one light curve corresponding to 'local_view'\n",
    "y = np.array([])\n",
    "for elem in parsed_dataset.take(1):\n",
    "  y = np.append(y,[elem['local_view']])\n",
    "length_lc = len(y)\n",
    "print(\"length_lc = \" ,length_lc)\n",
    "\n",
    "\n",
    "# Calculate total number of light curves\n",
    "no_data = 0\n",
    "for elem in parsed_dataset.as_numpy_iterator():\n",
    "    no_data = no_data + 1\n",
    "print('no_data = ',no_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Kepler ID\n",
    "\n",
    "kepid_array = np.zeros(shape=(no_data,))\n",
    "i = 0\n",
    "for elem in parsed_dataset:\n",
    "    kepid_array[i] = elem['kepid'] \n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(kepid_array[908:912])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all tf dataset to np array (light curve numpy array (lc_np_array))\n",
    "lc_np_array = np.zeros(shape=(no_data,length_lc))\n",
    "lc_np_array.shape\n",
    "\n",
    "\n",
    "i = 0\n",
    "for elem in parsed_dataset.as_numpy_iterator():\n",
    "    lc_np_array[i] = elem['local_view'] #+1.0\n",
    "    i = i + 1\n",
    "\n",
    "print('lc_np_array[0] = ',lc_np_array[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.arange(1):\n",
    "    i=8970\n",
    "    test_lc = lc_np_array[i]\n",
    "\n",
    "    ph = np.linspace(-1.0,1.0,len(lc_np_array[0]))\n",
    "    print('len(ph) = ',len(ph))\n",
    "    plt.scatter(ph[0:60], test_lc[0:60])\n",
    "    print('len(ph[0:60]) = ',len(ph[0:60]))\n",
    "    plt.scatter(ph[141:202], test_lc[141:202])\n",
    "    # print('len(ph[141:202]) = ',len(ph[141:202]))\n",
    "    plt.scatter(ph[60:141], test_lc[60:141])\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noise Array\n",
    "# Append first 60 and last 60 elements\n",
    "noise_array = np.zeros((len(lc_np_array),120))\n",
    "for i in np.arange(len(lc_np_array)):\n",
    "    noise_array[i][0:60] = lc_np_array[i,0:60]\n",
    "    noise_array[i][60:120] = lc_np_array[i,141:202]\n",
    "\n",
    "\n",
    "ph = np.linspace(-1.0,1.0,len(noise_array[0]))\n",
    "plt.scatter(ph, noise_array[0])\n",
    "print('len(ph) = ',len(ph))\n",
    "print('noise_array.shape = ',noise_array.shape)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calc std dev of the arrays\n",
    "std_devs_Kepler = np.array([np.std(arr) for arr in noise_array])\n",
    "\n",
    "# Plot histogram of std dev\n",
    "# bins = np.logspace(-5,5,100)\n",
    "# bins = np.linspace(0,10**2,11)\n",
    "bins = np.linspace(0,2,100)\n",
    "\n",
    "plt.figure(figsize=(9, 9))\n",
    "# plt.hist(std_devs_Kepler, bins=bins, density=True, alpha=0.8, color='tab:green')\n",
    "a,*_ = np.histogram(std_devs_Kepler, bins=bins)\n",
    "print('a = ',a)\n",
    "a_percent = (a/np.sum(a))*100\n",
    "print('a_percent = ',a_percent)\n",
    "\n",
    "plt.stairs(a_percent, bins, baseline=0,fill=True,color='black')\n",
    "plt.xlabel('Sigma')\n",
    "# plt.xscale(\"log\")\n",
    "plt.ylabel('Probability (%)')\n",
    "plt.title('Histogram - Standard Deviation')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add noise to train lightcurve\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add noise to simulated train light curve, such that index is uniform\n",
    "index_noise_train= np.random.uniform(0,len(noise_array),(len(train_lc_scaled_append),))\n",
    "train_lc_scaled_append_noise = np.zeros(train_lc_scaled_append.shape)\n",
    "print(\"index_noise_train.shape = \",index_noise_train.shape)\n",
    "i = 0\n",
    "for i in np.arange(len(train_lc_scaled_append)):\n",
    "    index = int(index_noise_train[i])\n",
    "    print(index)\n",
    "    train_lc_scaled_append_noise[i] = train_lc_scaled_append[i] + noise_array[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot - Train LCs\n",
    "num = 5\n",
    "fig,ax=plt.subplots(num,4, figsize=(10,10), gridspec_kw={ 'width_ratios': [2,3,3,3],\n",
    "        'wspace': 0.4,'hspace': 0.7})\n",
    "\n",
    "ph = np.linspace(-1,1,len(train_lc_scaled_append_noise[0]))\n",
    "ph_kepler = np.linspace(-1,1,len(lc_np_array[0]))\n",
    "\n",
    "i = 0\n",
    "for i in np.arange(0,num):\n",
    "    k = random.randint(0, len(train_lc_scaled_append_noise)-1)\n",
    "    original_kelper_index = int(index_noise_train[int(k)])\n",
    "\n",
    "\n",
    "    ax[i][0].set_title('Shape',size=13)\n",
    "    img = ax[i][0].imshow(train_shape[k],cmap='inferno')\n",
    "    plt.colorbar(img)\n",
    "\n",
    "    ax[i][1].set_title('Simulated LC',size=13)\n",
    "    ax[i][1].plot(ph, train_lc_scaled_append[k],color = 'black',linewidth='3')\n",
    "\n",
    "    ax[i][2].set_title(f'Kepler LC. std_dev = {np.round(std_devs_Kepler[original_kelper_index],4)}',size=13)\n",
    "    ax[i][2].plot(ph_kepler, lc_np_array[original_kelper_index]+1,color = 'black',linewidth='3')\n",
    "    \n",
    "    ax[i][3].set_title('Noise Added LC',size=13)\n",
    "    ax[i][3].plot(ph, train_lc_scaled_append_noise[k],color = 'black',linewidth='3')\n",
    "\n",
    "plt.suptitle(\"Train Set\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding noise to Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add noise to simulated vald light curve, such that index is uniform\n",
    "index_noise_vald= np.random.uniform(0,len(noise_array),(len(vald_lc_scaled_append),))\n",
    "vald_lc_scaled_append_noise = np.zeros(vald_lc_scaled_append.shape)\n",
    "print(\"index_noise_vald.shape = \",index_noise_vald.shape)\n",
    "i = 0\n",
    "for i in np.arange(len(vald_lc_scaled_append)):\n",
    "    index = int(index_noise_vald[i])\n",
    "    print(index)\n",
    "    vald_lc_scaled_append_noise[i] = vald_lc_scaled_append[i] + noise_array[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot - Vald LCs\n",
    "num = 5\n",
    "fig,ax=plt.subplots(num,4, figsize=(10,10), gridspec_kw={ 'width_ratios': [2,3,3,3],\n",
    "        'wspace': 0.4,'hspace': 0.7})\n",
    "\n",
    "ph = np.linspace(-1,1,len(vald_lc_scaled_append_noise[0]))\n",
    "ph_kepler = np.linspace(-1,1,len(lc_np_array[0]))\n",
    "\n",
    "i = 0\n",
    "for i in np.arange(0,num):\n",
    "    k = random.randint(0, len(vald_lc_scaled_append_noise)-1)\n",
    "    original_kelper_index = int(index_noise_vald[int(k)])\n",
    "\n",
    "\n",
    "    ax[i][0].set_title('Shape',size=13)\n",
    "    img = ax[i][0].imshow(vald_shape[k],cmap='inferno')\n",
    "    plt.colorbar(img)\n",
    "\n",
    "    ax[i][1].set_title('Simulated LC',size=13)\n",
    "    ax[i][1].plot(ph, vald_lc_scaled_append[k],color = 'black',linewidth='3')\n",
    "\n",
    "    ax[i][2].set_title(f'Kepler LC. std_dev = {np.round(std_devs_Kepler[original_kelper_index],4)}',size=13)\n",
    "    ax[i][2].plot(ph_kepler, lc_np_array[original_kelper_index]+1,color = 'black',linewidth='3')\n",
    "    \n",
    "    ax[i][3].set_title('Noise Added LC',size=13)\n",
    "    ax[i][3].plot(ph, vald_lc_scaled_append_noise[k],color = 'black',linewidth='3')\n",
    "\n",
    "plt.suptitle(\"Validation Set\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
