{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is used to overlap simulated light curve and \n",
    "# Kepler Lightcurve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to convert tfrecord to npy (Shallue's dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TF and check for GPU\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from tensorflow.keras.models import save_model, load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import math\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = ['../data/tfr_shallue/tfr_all/tfrecord/train-00000-of-00008',\n",
    "             '../data/tfr_shallue/tfr_all/tfrecord/train-00001-of-00008',\n",
    "             '../data/tfr_shallue/tfr_all/tfrecord/train-00002-of-00008',\n",
    "             '../data/tfr_shallue/tfr_all/tfrecord/train-00003-of-00008',\n",
    "             '../data/tfr_shallue/tfr_all/tfrecord/train-00004-of-00008',\n",
    "             '../data/tfr_shallue/tfr_all/tfrecord/train-00005-of-00008',\n",
    "             '../data/tfr_shallue/tfr_all/tfrecord/train-00006-of-00008',\n",
    "             '../data/tfr_shallue/tfr_all/tfrecord/train-00007-of-00008',\n",
    "             '../data/tfr_shallue/tfr_all/tfrecord/val-00000-of-00001',\n",
    "             '../data/tfr_shallue/tfr_all/tfrecord/test-00000-of-00001']\n",
    "raw_dataset = tf.data.TFRecordDataset(filenames)\n",
    "raw_dataset\n",
    "\n",
    "feature_description = {\n",
    "    'global_view': tf.io.FixedLenSequenceFeature([], tf.float32, default_value=0.0,allow_missing=True),\n",
    "    'local_view': tf.io.FixedLenSequenceFeature([], tf.float32, default_value=0.0,allow_missing=True),\n",
    "    'av_training_set': tf.io.FixedLenFeature([], tf.string, default_value=''),\n",
    "    'kepid': tf.io.FixedLenSequenceFeature([], tf.int64, default_value=int(0),allow_missing=True),\n",
    "    'tce_plnt_num': tf.io.FixedLenSequenceFeature([], tf.int64, default_value=int(0),allow_missing=True),\n",
    "    'tce_period': tf.io.FixedLenSequenceFeature([], tf.float32, default_value=int(0),allow_missing=True),\n",
    "}\n",
    "\n",
    "def _parse_function(example_proto):\n",
    "  # Parse the input `tf.train.Example` proto using the dictionary above.\n",
    "  return tf.io.parse_single_example(example_proto, feature_description)\n",
    "\n",
    "parsed_dataset = raw_dataset.map(_parse_function)\n",
    "parsed_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate total number of light curves\n",
    "# no_data = 0\n",
    "# for elem in parsed_dataset.as_numpy_iterator():\n",
    "#     no_data = no_data + 1\n",
    "# print('no_data = ',no_data)\n",
    "\n",
    "# # # Extract Row ID 'rowid'\n",
    "# # rowid_array = np.zeros(shape=(no_data,))\n",
    "# # iteration = 0\n",
    "# # for elem in parsed_dataset:\n",
    "# #     print(elem['rowid'])\n",
    "\n",
    "# iteration = 0\n",
    "# for elem in parsed_dataset:\n",
    "#     print(elem['kepid'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert TFRecord to npy file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total number of light curves\n",
    "no_data = 0\n",
    "for elem in parsed_dataset.as_numpy_iterator():\n",
    "    no_data = no_data + 1\n",
    "print('no_data = ',no_data)\n",
    "\n",
    "# global_view\n",
    "# Calculate the size of one light curve corresponding to 'global_view'\n",
    "y = np.array([])\n",
    "for elem in parsed_dataset.take(1):\n",
    "  y = np.append(y,[elem['global_view']])\n",
    "length_global_view = len(y)\n",
    "print(\"length_global_view = \" ,length_global_view)\n",
    "\n",
    "global_view_array = np.zeros(shape=(no_data,length_global_view))\n",
    "global_view_array.shape\n",
    "\n",
    "iteration = 0\n",
    "for elem in parsed_dataset.as_numpy_iterator():\n",
    "    global_view_array[iteration] = elem['global_view'] \n",
    "    iteration = iteration + 1\n",
    "\n",
    "# local_view\n",
    "y = np.array([])\n",
    "for elem in parsed_dataset.take(1):\n",
    "  y = np.append(y,[elem['local_view']])\n",
    "length_local_view = len(y)\n",
    "print(\"length_local_view = \" ,length_local_view)\n",
    "\n",
    "local_view_array = np.zeros(shape=(no_data,length_local_view))\n",
    "local_view_array.shape\n",
    "\n",
    "iteration = 0\n",
    "for elem in parsed_dataset.as_numpy_iterator():\n",
    "    local_view_array[iteration] = elem['local_view'] \n",
    "    iteration = iteration + 1\n",
    "\n",
    "# Extract Kepler ID 'kepid'\n",
    "kepid_array = np.zeros(shape=(no_data,))\n",
    "iteration = 0\n",
    "for elem in parsed_dataset:\n",
    "    kepid_array[iteration] = elem['kepid'] \n",
    "    iteration = iteration + 1\n",
    "print(\"First element in kepid_array = \",kepid_array[0])\n",
    "# Extract Row ID 'rowid'\n",
    "# rowid_array = np.zeros(shape=(no_data,))\n",
    "# iteration = 0\n",
    "# for elem in parsed_dataset:\n",
    "#     rowid_array[iteration] = elem['rowid'] \n",
    "#     iteration = iteration + 1\n",
    "    \n",
    "# 'av_training_set'\n",
    "av_training_set_array = np.chararray(shape=(no_data,))\n",
    "iteration = 0\n",
    "for elem in parsed_dataset.as_numpy_iterator():\n",
    "    av_training_set_array[iteration] = elem['av_training_set']\n",
    "    iteration = iteration + 1\n",
    "print(\"First element in av_training_set_array = \",av_training_set_array[0])\n",
    "\n",
    "# # 'tce_plnt_num'\n",
    "# tce_plnt_num_array = np.zeros(shape=(no_data,))\n",
    "# iteration = 0\n",
    "# for elem in parsed_dataset.as_numpy_iterator():\n",
    "#     tce_plnt_num_array[iteration] = elem['tce_plnt_num']\n",
    "#     iteration = iteration + 1\n",
    "# print(\"First element in tce_plnt_num_array = \",tce_plnt_num_array[0])\n",
    "\n",
    "# # tce_period\n",
    "# tce_period_array = np.zeros(shape=(no_data,))\n",
    "# iteration = 0\n",
    "# for elem in parsed_dataset.as_numpy_iterator():\n",
    "#     tce_period_array[iteration] = elem['tce_period']\n",
    "#     iteration = iteration + 1\n",
    "# print(\"First element in tce_period_array = \",tce_period_array[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFR data as npy file\n",
    "# np.save(\"../data/tfr_shallue/npy_shallue/av_training_set.npy\", av_training_set_array)\n",
    "\n",
    "np.save(\"../data/tfr_shallue/npy_shallue/global_view.npy\", global_view_array)\n",
    "np.save(\"../data/tfr_shallue/npy_shallue/local_view.npy\", local_view_array)\n",
    "np.save(\"../data/tfr_shallue/npy_shallue/kepid.npy\", kepid_array)\n",
    "# np.save(\"../data/tfr_shallue/npy_shallue/tce_plnt_num.npy\", tce_plnt_num_array)\n",
    "# np.save(\"../data/tfr_shallue/npy_shallue/tce_period.npy\", tce_period_array)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kepid_load = np.load(\"../data/tfr_shallue/npy_shallue/kepid.npy\")\n",
    "print('kepid_load = ',kepid_load[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = ['../data/tfr_shallue/tfr_all/tfrecord/train-00000-of-00008',\n",
    "             '../data/tfr_shallue/tfr_all/tfrecord/train-00001-of-00008',\n",
    "             '../data/tfr_shallue/tfr_all/tfrecord/train-00002-of-00008',\n",
    "             '../data/tfr_shallue/tfr_all/tfrecord/train-00003-of-00008',\n",
    "             '../data/tfr_shallue/tfr_all/tfrecord/train-00004-of-00008',\n",
    "             '../data/tfr_shallue/tfr_all/tfrecord/train-00005-of-00008',\n",
    "             '../data/tfr_shallue/tfr_all/tfrecord/train-00006-of-00008',\n",
    "             '../data/tfr_shallue/tfr_all/tfrecord/train-00007-of-00008',\n",
    "             '../data/tfr_shallue/tfr_all/tfrecord/val-00000-of-00001',\n",
    "             '../data/tfr_shallue/tfr_all/tfrecord/test-00000-of-00001']\n",
    "raw_dataset = tf.data.TFRecordDataset(filenames)\n",
    "raw_dataset\n",
    "\n",
    "feature_description = {\n",
    "    'global_view': tf.io.FixedLenSequenceFeature([], tf.float32, default_value=0.0,allow_missing=True),\n",
    "    'local_view': tf.io.FixedLenSequenceFeature([], tf.float32, default_value=0.0,allow_missing=True),\n",
    "    'av_training_set': tf.io.FixedLenFeature([], tf.string, default_value=''),\n",
    "    'kepid': tf.io.FixedLenSequenceFeature([], tf.int64, default_value=int(0),allow_missing=True),\n",
    "}\n",
    "\n",
    "def _parse_function(example_proto):\n",
    "  # Parse the input `tf.train.Example` proto using the dictionary above.\n",
    "  return tf.io.parse_single_example(example_proto, feature_description)\n",
    "\n",
    "# At this point the dataset contains serialized tf.train.Example messages.\n",
    "# When iterated over it returns these as scalar string tensors.\n",
    "# Use the .take method to only show the first 10 records.\n",
    "for raw_record in raw_dataset.take(10):\n",
    "  print(repr(raw_record))\n",
    "  \n",
    "parsed_dataset = raw_dataset.map(_parse_function)\n",
    "parsed_dataset\n",
    "\n",
    "\n",
    "# Calculate the size of one light curve corresponding to 'local_view'\n",
    "y = np.array([])\n",
    "for elem in parsed_dataset.take(1):\n",
    "  y = np.append(y,[elem['local_view']])\n",
    "length_lc = len(y)\n",
    "print(\"length_lc = \" ,length_lc)\n",
    "\n",
    "\n",
    "# Calculate total number of light curves\n",
    "no_data = 0\n",
    "for elem in parsed_dataset.as_numpy_iterator():\n",
    "    no_data = no_data + 1\n",
    "print('no_data = ',no_data)\n",
    "\n",
    "# Extract Kepler ID\n",
    "kepid_array = np.zeros(shape=(no_data,))\n",
    "i = 0\n",
    "for elem in parsed_dataset:\n",
    "    kepid_array[i] = elem['kepid'] \n",
    "    i = i + 1\n",
    "\n",
    "# lc_label_array - array containing labels\n",
    "lc_label_array = np.chararray(shape=(no_data,))\n",
    "\n",
    "i = 0\n",
    "for elem in parsed_dataset.as_numpy_iterator():\n",
    "    lc_label_array[i] = elem['av_training_set']\n",
    "    i = i + 1\n",
    "\n",
    "# convert all tf dataset to np array (light curve numpy array (lc_np_array))\n",
    "lc_np_array = np.zeros(shape=(no_data,length_lc))\n",
    "lc_np_array.shape\n",
    "\n",
    "i = 0\n",
    "for elem in parsed_dataset.as_numpy_iterator():\n",
    "    lc_np_array[i] = elem['local_view'] #+1.0\n",
    "    i = i + 1\n",
    "\n",
    "print('lc_np_array[0] = ',lc_np_array[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.arange(1):\n",
    "    j=981\n",
    "    test_lc = lc_np_array[j]\n",
    "    ph = np.linspace(-1.0,1.0,len(lc_np_array[0]))\n",
    "    plt.scatter(ph, test_lc)\n",
    "    print(int(kepid_array[j]))\n",
    "    plt.title(f\"{int(kepid_array[j])}| {lc_label_array[j]}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to select all the index of Kepler light curves with SNR > 50\n",
    "\n",
    "# Calculate SNR of all the lc in Kepler Dataset\n",
    "lc_np_array_offset = lc_np_array + 1\n",
    "noise_array = np.zeros((len(lc_np_array),120))\n",
    "for i in np.arange(len(lc_np_array)):\n",
    "    noise_array[i][0:60] = lc_np_array[i,0:60]\n",
    "    noise_array[i][60:120] = lc_np_array[i,141:202]\n",
    "std_devs_Kepler = np.array([np.std(arr) for arr in noise_array])\n",
    "SNR_Kepler = 1/std_devs_Kepler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to show the properties of the light curves in Kepler Dataset\n",
    "print(f\"Total no. of light curves in Kepler Dataset = \",len(SNR_Kepler))\n",
    "print(\"\\tTotal No. of planets = \",len(np.where(lc_label_array==b'P')[0]))\n",
    "print(\"\\tTotal no. of non-transiting phenomenon = \",len(np.where(lc_label_array==b'N')[0]))\n",
    "print(\"\\tTotal no. of astrophysical false positive = \",len(np.where(lc_label_array==b'A')[0]))\n",
    "print(\"\\n---\\n\")\n",
    "SNR_Threshold_array = [50,75,100,500]\n",
    "for SNR_Threshold in SNR_Threshold_array:\n",
    "    selected_kepler_index_mask = SNR_Kepler > SNR_Threshold\n",
    "    selected_kepler_index = np.where(selected_kepler_index_mask)[0]\n",
    "    print(f\"For SNR >  {SNR_Threshold}\")\n",
    "    print(f\"\\tNo. of light curves = \",len(selected_kepler_index))\n",
    "    # print(\"Index where label = Planet: \",np.where(lc_label_array[selected_kepler_index]==b'P')[0])\n",
    "    print(\"\\tNo. of planets = \",len(np.where(lc_label_array[selected_kepler_index]==b'P')[0]))\n",
    "    print(\"\\tNo. of non-transiting phenomenon = \",len(np.where(lc_label_array[selected_kepler_index]==b'N')[0]))\n",
    "    print(\"\\tNo. of astrophysical false positive = \",len(np.where(lc_label_array[selected_kepler_index]==b'A')[0]))\n",
    "\n",
    "    # print('selected_kepler_index_mask = ',selected_kepler_index_mask)\n",
    "    # print('selected_kepler_index = ',selected_kepler_index)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the index of the light curve with SNR > 50\n",
    "# select_kepler_lc = lc_np_array[SNR_Kepler>75]\n",
    "# select_kepler_lc = select_kepler_lc + 1\n",
    "\n",
    "SNR_Threshold = 500\n",
    "selected_kepler_index_mask = (SNR_Kepler > SNR_Threshold)  # & (SNR_Kepler < 500) \n",
    "selected_kepler_index = np.where(selected_kepler_index_mask)[0]\n",
    "print(f\"No. of light curves with SNR > {SNR_Threshold} = \",len(selected_kepler_index))\n",
    "print(f\"Total no. of light curves = \",len(SNR_Kepler))\n",
    "# Plot the selected light curves 3 x 3\n",
    "num = 3\n",
    "fig,ax=plt.subplots(num,3, figsize=(8,6), gridspec_kw={ 'width_ratios': [1,1,1],\n",
    "        'wspace': 0.2,'hspace': 0.4})\n",
    "\n",
    "# ax[0][1].set_title('Shape',size=15)\n",
    "# ax[0][0].set_title('Light Curve (Train Dataset)',size=15)\n",
    "# ax[num-1][0].set_xlabel('Phase',size=13)\n",
    "ph_kepler = np.linspace(-1,1,len(lc_np_array_offset[0]))\n",
    "\n",
    "\n",
    "i = 0\n",
    "for i in np.arange(0,num):\n",
    "    # k = np.random.randint(0, len(select_kepler_lc)-50)\n",
    "    k = int(np.random.choice(selected_kepler_index))\n",
    "    ax[i][0].set_title(f'SNR = {int(np.round(SNR_Kepler[k],0))} | {lc_label_array[k]}',size=13)\n",
    "    ax[i][0].set_ylim(-0.5,1.5)\n",
    "    ax[i][0].plot(ph_kepler, lc_np_array_offset[k],color = 'tab:red',linewidth='2')\n",
    "    ax[i][0].grid('on')\n",
    "\n",
    "    k = int(np.random.choice(selected_kepler_index))\n",
    "    ax[i][1].set_title(f'SNR = {int(np.round(SNR_Kepler[k],0))} | {lc_label_array[k]}',size=13)\n",
    "    ax[i][1].set_ylim(-0.5,1.5)\n",
    "    ax[i][1].plot(ph_kepler, lc_np_array_offset[k],color = 'tab:red',linewidth='2')\n",
    "    ax[i][1].grid('on')\n",
    "\n",
    "    k = int(np.random.choice(selected_kepler_index))\n",
    "    ax[i][2].set_title(f'SNR = {int(np.round(SNR_Kepler[k],0))} | {lc_label_array[k]}',size=13)\n",
    "    ax[i][2].set_ylim(-0.5,1.5)\n",
    "    ax[i][2].plot(ph_kepler, lc_np_array_offset[k],color = 'tab:red',linewidth='2')\n",
    "    ax[i][2].grid('on')\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code to shift the light curve to the centre\n",
    "\n",
    "# Check whether any index for phase axis phase is 0\n",
    "print('np.where(ph_kepler == 0)[0] = ', np.where(ph_kepler == 0)[0]) # OP: np.where(ph == 0)[0] =  [100]\n",
    "print('len(ph_kepler) = ',len(ph_kepler)) # OP: len(ph) =  201\n",
    "\n",
    "# Calculate the index where light curve is at minimum and shift it\n",
    "min_index = np.zeros(len(lc_np_array_offset))\n",
    "for iter in np.arange(len(lc_np_array_offset)):\n",
    "    min_index[iter] = np.argmin(lc_np_array_offset[iter])\n",
    "    # Calculate the number of positions to shift the array\n",
    "    shift_amount = len(lc_np_array_offset[iter]) // 2 - min_index[iter]\n",
    "\n",
    "    # Shift the array to bring the minimum value to the central index\n",
    "    lc_np_array_offset[iter] = np.roll(lc_np_array_offset[iter], int(shift_amount))\n",
    "    min_index[iter] = np.argmin(lc_np_array_offset[iter])\n",
    "\n",
    "# Plot the histogram of the index\n",
    "bins = np.linspace(0,201,202)\n",
    "\n",
    "plt.figure(figsize=(9, 9))\n",
    "# plt.hist(std_devs_Kepler, bins=bins, density=True, alpha=0.8, color='tab:green')\n",
    "a,*_ = np.histogram(min_index, bins=bins)\n",
    "# print('a = ',a)\n",
    "print('np.sum(a) = ', np.sum(a))\n",
    "a_percent = (a/np.sum(a))*100\n",
    "# print('a_percent = ',a_percent)\n",
    "\n",
    "plt.stairs(a_percent, bins, baseline=0,fill=True,color='black')\n",
    "plt.xlabel('Index')\n",
    "# plt.xscale(\"log\")\n",
    "plt.ylabel('Probability (%)')\n",
    "plt.title('Histogram - Index at where minimum occurs')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To plot the shifted lightcurves\n",
    "\n",
    "print(f\"Total no. of light curves = \",len(SNR_Kepler))\n",
    "# Plot the selected light curves 3 x 3\n",
    "num = 3\n",
    "fig,ax=plt.subplots(num,3, figsize=(8,6), gridspec_kw={ 'width_ratios': [1,1,1],\n",
    "        'wspace': 0.2,'hspace': 0.4})\n",
    "\n",
    "# ax[0][1].set_title('Shape',size=15)\n",
    "# ax[0][0].set_title('Light Curve (Train Dataset)',size=15)\n",
    "# ax[num-1][0].set_xlabel('Phase',size=13)\n",
    "ph_kepler = np.linspace(-1,1,len(lc_np_array_offset[0]))\n",
    "\n",
    "\n",
    "i = 0\n",
    "for i in np.arange(0,num):\n",
    "    # k = np.random.randint(0, len(select_kepler_lc)-50)\n",
    "    k = int(np.random.choice(selected_kepler_index))\n",
    "    ax[i][0].set_title(f'SNR = {int(np.round(SNR_Kepler[k],0))} | {lc_label_array[k]}',size=13)\n",
    "    ax[i][0].set_ylim(-0.5,1.5)\n",
    "    ax[i][0].plot(ph_kepler, lc_np_array_offset[k],color = 'tab:red',label='shifted',linewidth='2')\n",
    "    ax[i][0].plot(ph_kepler, lc_np_array[k]+1,color = 'tab:blue',label='original',linewidth='2')\n",
    "    ax[i][0].grid('on')\n",
    "#     ax[i][0].legend()\n",
    "\n",
    "\n",
    "    k = int(np.random.choice(selected_kepler_index))\n",
    "    ax[i][1].set_title(f'SNR = {int(np.round(SNR_Kepler[k],0))} | {lc_label_array[k]}',size=13)\n",
    "    ax[i][1].set_ylim(-0.5,1.5)\n",
    "    ax[i][1].plot(ph_kepler, lc_np_array_offset[k],color = 'tab:red',label='shifted',linewidth='2')\n",
    "    ax[i][1].plot(ph_kepler, lc_np_array[k]+1,color = 'tab:blue',label='original',linewidth='2')\n",
    "    ax[i][1].grid('on')\n",
    "#     ax[i][1].legend()\n",
    "\n",
    "\n",
    "    k = int(np.random.choice(selected_kepler_index))\n",
    "    ax[i][2].set_title(f'SNR = {int(np.round(SNR_Kepler[k],0))} | {lc_label_array[k]}',size=13)\n",
    "    ax[i][2].set_ylim(-0.5,1.5)\n",
    "    ax[i][2].plot(ph_kepler, lc_np_array_offset[k],color = 'tab:red',label='shifted',linewidth='2')\n",
    "    ax[i][2].plot(ph_kepler, lc_np_array[k]+1,color = 'tab:blue',label='original',linewidth='2')\n",
    "    ax[i][2].grid('on')\n",
    "    ax[i][2].legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.logspace(0,5,60)\n",
    "\n",
    "plt.figure(figsize=(9, 9))\n",
    "# plt.hist(std_devs_Kepler, bins=bins, density=True, alpha=0.8, color='tab:green')\n",
    "a,*_ = np.histogram(SNR_Kepler, bins=bins)\n",
    "# print('a = ',a)\n",
    "print('np.sum(a) = ', np.sum(a))\n",
    "a_percent = (a/np.sum(a))*100\n",
    "# print('a_percent = ',a_percent)\n",
    "\n",
    "plt.stairs(a_percent, bins, baseline=0,fill=True,color='black')\n",
    "plt.xlabel('SNR')\n",
    "plt.xscale(\"log\")\n",
    "plt.ylabel('Probability (%)')\n",
    "plt.title('Histogram - SNR')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load simulation light curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load Dataset\n",
    "## Load Train Set\n",
    "# train_shape_dir = '../data/data_npy/shape_npy/shape_filled5.npy'\n",
    "# train_lc_dir = '../data/data_npy/lc_npy/lc_dict_5.npy'\n",
    "\n",
    "train_shape_dir = '../data/train/npy/shape/shape_5.npy'\n",
    "train_lc_dir =  '../data/train/npy/lc/lc_1_shape_5.npy'\n",
    "# train_lc_dir =  '../data/train/npy/lc/lc_2_shape_5.npy'\n",
    "train_lc = np.load(train_lc_dir)\n",
    "train_shape = np.load(train_shape_dir)\n",
    "# Check equality of number of dataset\n",
    "if len(train_lc)==len(train_shape):\n",
    "    print(\"Train Set: No. of LC = No. of shapes\")\n",
    "else:\n",
    "    sys.exit(\"EXIT: Train Set: No. of LC != No. of shapes\")\n",
    "\n",
    "## Load Validation Set\n",
    "# vald_shape_dir = '../data/data_npy/shape_npy/shape_filled4.npy'\n",
    "# vald_lc_dir = '../data/data_npy/lc_npy/lc_dict_4.npy'\n",
    "\n",
    "vald_shape_dir = '../data/vald/npy/shape/shape_1.npy'\n",
    "vald_lc_dir = '../data/vald/npy/lc/lc_1_shape_1.npy'\n",
    "\n",
    "vald_lc = np.load(vald_lc_dir)\n",
    "vald_shape = np.load(vald_shape_dir)\n",
    "# Check equality of nuftmber of dataset\n",
    "if len(vald_lc)==len(vald_shape):\n",
    "    print(\"Vald Set: No. of LC = No. of shapes\")\n",
    "else:\n",
    "    sys.exit(\"Vald Set: No. of LC = No. of shapes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Normalize the image, convert to opacity map\n",
    "## Train Set\n",
    "train_shape = train_shape/np.amax(train_shape)\n",
    "train_shape_where_0 = np.where(train_shape == 0)\n",
    "train_shape_where_1 = np.where(train_shape == 1)\n",
    "train_shape[train_shape_where_0] = 1  # 1 represent the shape (1 opacity)\n",
    "train_shape[train_shape_where_1] = 0  # 0 represent background (0 opacity)\n",
    "\n",
    "## Valdn Set\n",
    "vald_shape = vald_shape/np.amax(vald_shape)\n",
    "vald_shape_where_0 = np.where(vald_shape == 0)\n",
    "vald_shape_where_1 = np.where(vald_shape == 1)\n",
    "vald_shape[vald_shape_where_0] = 1  # 1 represent the shape (1 opacity)\n",
    "vald_shape[vald_shape_where_1] = 0  # 0 represent background (0 opacity)\n",
    "print(\"Normalized the shape\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Normalize the lightcurves\n",
    "## - Train Set\n",
    "train_lc_scaled = np.zeros(train_lc.shape)\n",
    "for i in np.arange(len(train_lc_scaled)):\n",
    "    train_lc_scaled[i] = (train_lc[i] - np.amin(train_lc[i]))/(np.amax(train_lc[i]) - np.amin(train_lc[i]))\n",
    "\n",
    "## - Vald Set\n",
    "vald_lc_scaled = np.zeros(vald_lc.shape)\n",
    "for i in np.arange(len(vald_lc_scaled)):\n",
    "    vald_lc_scaled[i] = (vald_lc[i] - np.amin(vald_lc[i]))/(np.amax(vald_lc[i]) - np.amin(vald_lc[i]))\n",
    "print(\"Normalized the light curves\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add flat line towards left and right of dip\n",
    "# 10 data points on each side\n",
    "# 3. Extend the lightcurves\n",
    "## - Train Set\n",
    "train_lc_scaled_append = np.ones((train_lc.shape[0],120))\n",
    "print('train_lc_scaled_append.shape = ',train_lc_scaled_append.shape)\n",
    "print(\"len(train_lc_scaled_append[0,10:110]) = \",len(train_lc_scaled_append[0,10:110]))\n",
    "\n",
    "for i in np.arange(len(train_lc_scaled)):\n",
    "    train_lc_scaled_append[i,10:110] = train_lc_scaled[i]\n",
    "\n",
    "## - Vald Set\n",
    "vald_lc_scaled_append = np.ones((vald_lc.shape[0],120))\n",
    "for i in np.arange(len(vald_lc_scaled)):\n",
    "    vald_lc_scaled_append[i,10:110] = vald_lc_scaled[i]\n",
    "print(\"Extended the light curves\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_lc_scaled_append\n",
    "del vald_lc_scaled_append\n",
    "\n",
    "train_lc_scaled_append = train_lc_scaled\n",
    "vald_lc_scaled_append = vald_lc_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "# Plot - Train LCs\n",
    "num = 3\n",
    "fig,ax=plt.subplots(num,2, figsize=(7,5), gridspec_kw={ 'width_ratios': [1,2],\n",
    "        'wspace': 0.4,'hspace': 0.4})\n",
    "plt.rcParams['figure.dpi'] = 400\n",
    "\n",
    "ax[0][0].set_title('Shape',size=13)\n",
    "ax[0][1].set_title('Light Curve',size=13)\n",
    "ax[num-1][1].set_xlabel('Phase (Arbitary Unit)',size=13)\n",
    "ph_simul = np.linspace(-1,1,len(train_lc_scaled_append[0]))\n",
    "# advance = 60\n",
    "\n",
    "i = 0\n",
    "for i in np.arange(0,num):\n",
    "    k = np.random.randint(0, len(train_lc_scaled_append)-1)\n",
    "    ax[i][0].tick_params(left = False, right = False , labelleft = False ,labelbottom = False, bottom = False)\n",
    "    if(i<num-1): ax[i][1].tick_params(labelbottom = False, bottom = False)\n",
    "    img = ax[i][0].imshow(train_shape[k],cmap='inferno')\n",
    "    plt.colorbar(img)\n",
    "    ax[i][1].set_ylabel('Flux',size=13)\n",
    "    ax[i][1].set_ylim(-0.5,1.5)\n",
    "    # ax[i][0].scatter(ph, vald_lc_scaled_append[k],color = 'black',marker='.')\n",
    "    ax[i][1].plot(ph_simul, train_lc_scaled_append[k],color = 'black',linewidth='1.5')\n",
    "    ax[i][1].grid('on')\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "# Plot - vald LCs\n",
    "num = 3\n",
    "fig,ax=plt.subplots(num,2, figsize=(4,3), gridspec_kw={ 'width_ratios': [2,1],\n",
    "        'wspace': 0.2,'hspace': 0.4})\n",
    "\n",
    "ax[0][1].set_title('Shape',size=15)\n",
    "ax[0][0].set_title('Light Curve (vald Dataset)',size=15)\n",
    "ax[num-1][0].set_xlabel('Phase',size=13)\n",
    "ph = np.linspace(-1,1,len(vald_lc_scaled_append[0]))\n",
    "# advance = 60\n",
    "\n",
    "i = 0\n",
    "for i in np.arange(0,num):\n",
    "    k = np.random.randint(0, len(vald_lc_scaled_append)-1)\n",
    "    ax[i][1].tick_params(left = False, right = False , labelleft = False ,labelbottom = False, bottom = False)\n",
    "    if(i<num-1): ax[i][0].tick_params(labelbottom = False, bottom = False)\n",
    "    img = ax[i][1].imshow(vald_shape[k],cmap='inferno')\n",
    "    plt.colorbar(img)\n",
    "    ax[i][0].set_ylabel('Flux',size=13)\n",
    "    ax[i][0].set_ylim(-0.5,1.5)\n",
    "#     ax[i][0].scatter(ph, vald_lc_scaled_append[k],color = 'black',marker='.')\n",
    "    ax[i][0].plot(ph_simul, vald_lc_scaled_append[k],color = 'tab:red',linewidth='2')\n",
    "    ax[i][0].grid('on')\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ph_simul,train_lc_scaled_append[0],label='Simulated LC')\n",
    "plt.plot(ph_kepler,lc_np_array_offset[selected_kepler_index[250]],label=f\"{kepid_array[selected_kepler_index[0]]}\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lc_np_array_offset_mask used to select the flat part by certain percentage\n",
    "lc_np_array_offset_mask = np.copy(lc_np_array_offset)\n",
    "\n",
    "for iter in np.arange(len(lc_np_array_offset_mask)):\n",
    "    lc_np_array_offset_mask[iter][(lc_np_array_offset_mask[iter]>=0.988)] = 1.0\n",
    "    lc_np_array_offset_mask[iter][(lc_np_array_offset_mask[iter]<0.988)] = 0.0\n",
    "\n",
    "print(\"Length of one Kepler LC = \",len(lc_np_array_offset_mask[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ph_simul,train_lc_scaled_append[0],label='Simulated LC')\n",
    "plt.plot(ph_kepler,lc_np_array_offset[selected_kepler_index[0]],label=f\"{kepid_array[selected_kepler_index[0]]}\")\n",
    "plt.plot(ph_kepler,lc_np_array_offset_mask[selected_kepler_index[0]],label=f\"{kepid_array[selected_kepler_index[0]]}\")\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_np_array_interpol = np.zeros((len(lc_np_array_offset),len(train_lc_scaled_append[0])))\n",
    "print(lc_np_array_interpol.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iter in np.arange(len(lc_np_array_interpol)):\n",
    "    # Calculate the number of occurrences of '0'\n",
    "    count_zeros = np.count_nonzero(lc_np_array_offset_mask[iter] == 0)\n",
    "    print(\"Number of '0's in the array:\", count_zeros)\n",
    "    \n",
    "    horiz_select = int(np.round(count_zeros/6))\n",
    "    print(\"No. of 1's to include in each side = \",horiz_select)\n",
    "\n",
    "    \n",
    "    # Find the index of the minimum value\n",
    "    min_index_lc_np_array_offset = np.argmin(lc_np_array_offset)\n",
    "    print('min_index for lc_np_array_offset',min_index_lc_np_array_offset)\n",
    "\n",
    "    selected_portion = lc_np_array_offset[iter][int(min_index_lc_np_array_offset-(count_zeros/2)-(horiz_select)):int(min_index_lc_np_array_offset+(count_zeros/2)+(horiz_select))]\n",
    "    selected_portion_mask = lc_np_array_offset_mask[iter][int(min_index_lc_np_array_offset-(count_zeros/2)-(horiz_select)):int(min_index_lc_np_array_offset+(count_zeros/2)+(horiz_select))]\n",
    "    \n",
    "    print('len_of_selected_portion = ',len(selected_portion))\n",
    "\n",
    "    # plt.plot(np.linspace(0,1,len(selected_portion)),selected_portion)\n",
    "    # plt.plot(np.linspace(0,1,len(train_lc_scaled_append[0])),train_lc_scaled_append[0])\n",
    "    # plt.plot(np.linspace(0,1,len(selected_portion_mask)),selected_portion_mask)\n",
    "    # plt.plot(np.linspace(0,1,len(lc_np_array_offset[selected_kepler_index[iter]])),lc_np_array_offset[selected_kepler_index[iter]])\n",
    "    # plt.plot(np.linspace(0,1,len(lc_np_array_offset_mask[selected_kepler_index[iter]])),lc_np_array_offset_mask[selected_kepler_index[iter]])\n",
    "\n",
    "    # Interpolate the selected portion\n",
    "    # Original data with 30 elements\n",
    "    original_x = np.linspace(0, 100, num=len(selected_portion))\n",
    "    original_y = selected_portion  # Replace with your actual data\n",
    "\n",
    "    # Create a quadratic interpolation function\n",
    "    f = interp1d(original_x, original_y, kind='quadratic')\n",
    "\n",
    "    # Define the range of x-values for the interpolation with 120 elements\n",
    "    x_interpolation = np.linspace(0, 100, num=len(train_lc_scaled_append[0]))\n",
    "\n",
    "    # Perform the interpolation\n",
    "    y_interpolated = f(x_interpolation)\n",
    "    lc_np_array_interpol[iter] = y_interpolated\n",
    "    \n",
    "plt.plot(np.linspace(0,1,len(lc_np_array_interpol[selected_kepler_index[0]])),lc_np_array_interpol[selected_kepler_index[0]],color='black')\n",
    "plt.grid('on')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter = 834\n",
    "plt.plot(np.linspace(0,1,len(lc_np_array_interpol[selected_kepler_index[iter]])),lc_np_array_interpol[selected_kepler_index[iter]],color='black',label=\"Kepler LC\")\n",
    "plt.plot(np.linspace(0,1,len(train_lc_scaled_append[19865])),train_lc_scaled_append[0],color='red',label=\"Simulated LC\")\n",
    "plt.grid('on')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(len(lc_np_array_interpol[selected_kepler_index[iter]]))\n",
    "print(np.argmin(train_lc_scaled_append[19865]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 3x3 grid of subplots\n",
    "fig, axes = plt.subplots(3, 3, figsize=(10, 10))\n",
    "plt.suptitle(\"LDCs = 0.1,0.05\")\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        rand_select_simul = np.random.randint(len(train_lc_scaled_append))\n",
    "        rand_select_kepler = np.random.randint(len(selected_kepler_index))\n",
    "        ax = axes[i, j]\n",
    "        array_to_plot = lc_np_array_interpol[selected_kepler_index[rand_select_kepler]]\n",
    "        ax.plot(np.linspace(0,100,len(array_to_plot)),array_to_plot,label='Kepler (interpolated)')\n",
    "        ax.plot(np.linspace(0,100,len(train_lc_scaled_append[rand_select_simul])),train_lc_scaled_append[rand_select_simul],label='Simulated LC')\n",
    "    axes[i][2].legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for iter in np.array([100]):\n",
    "\n",
    "# for iter in np.arange(len(lc_np_array_interpol)):\n",
    "#     # count_ones_left = np.count_nonzero(lc_np_array_offset_mask[selected_kepler_index[iter]][:100] == 1)\n",
    "    \n",
    "#     # Calculate the number of occurrences of '0'\n",
    "#     count_zeros = np.count_nonzero(lc_np_array_offset_mask[selected_kepler_index[iter]] == 0)\n",
    "#     print(\"Number of '0's in the array:\", count_zeros)\n",
    "    \n",
    "#     horiz_select_left_len = int(np.round(count_zeros/5))\n",
    "#     horiz_select_right_len = horiz_select_left_len\n",
    "\n",
    "#     # count_ones_right = np.count_nonzero(lc_np_array_offset_mask[selected_kepler_index[iter]][101:] == 1)\n",
    "#     # horiz_select_right_len = int(np.round(count_ones_right/6))\n",
    "#     print(\"No. of 1's to include in left side from the first 0 = \",horiz_select_left_len)\n",
    "#     print(\"No. of 1's to include in right side from the first 0 = \",horiz_select_right_len)\n",
    "#     # Find the first index of '0'\n",
    "#     # first_index = int(np.argmax(lc_np_array_offset_mask[selected_kepler_index[iter]] == 0))\n",
    "\n",
    "#     # Find the last index of '0'\n",
    "#     # last_index = len(lc_np_array_offset_mask[selected_kepler_index[iter]]) - np.argmax(lc_np_array_offset[selected_kepler_index[iter]][::-1] == 0) - 1\n",
    "#     # Find the last index of '0'\n",
    "#     # indices = np.where(lc_np_array_offset_mask[selected_kepler_index[iter]] == 0)[0]\n",
    "#     # last_index = indices[-1] if indices.size > 0 else -1\n",
    "#     # print(\"Last index of '0':\", last_index)\n",
    "#     # print('first_index = ',first_index)\n",
    "#     # print('last_index = ',last_index)\n",
    "#         # selected_portion = lc_np_array_offset[selected_kepler_index[iter]][(first_index-horiz_select_left_len):(last_index+(horiz_select_right_len))]\n",
    "#     # selected_portion_mask = lc_np_array_offset_mask[selected_kepler_index[iter]][(first_index-horiz_select_left_len):(last_index+(horiz_select_right_len))]\n",
    "    \n",
    "#     # Find the index of the minimum value\n",
    "#     min_index_lc_np_array_offset = np.argmin(lc_np_array_offset)\n",
    "#     print('min_index for lc_np_array_offset',min_index_lc_np_array_offset)\n",
    "\n",
    "#     selected_portion = lc_np_array_offset[selected_kepler_index[iter]][int(min_index_lc_np_array_offset-(count_zeros/2)-(horiz_select_left_len)):int(min_index_lc_np_array_offset+(count_zeros/2)+(horiz_select_right_len))]\n",
    "#     selected_portion_mask = lc_np_array_offset_mask[selected_kepler_index[iter]][int(min_index_lc_np_array_offset-(count_zeros/2)-(horiz_select_left_len)):int(min_index_lc_np_array_offset+(count_zeros/2)+(horiz_select_right_len))]\n",
    "    \n",
    "#     print('len_of_selected_portion = ',len(selected_portion))\n",
    "\n",
    "#     plt.plot(np.linspace(0,1,len(selected_portion)),selected_portion)\n",
    "#     plt.plot(np.linspace(0,1,len(train_lc_scaled_append[0])),train_lc_scaled_append[0])\n",
    "#     # plt.plot(np.linspace(0,1,len(selected_portion_mask)),selected_portion_mask)\n",
    "#     # plt.plot(np.linspace(0,1,len(lc_np_array_offset[selected_kepler_index[iter]])),lc_np_array_offset[selected_kepler_index[iter]])\n",
    "#     # plt.plot(np.linspace(0,1,len(lc_np_array_offset_mask[selected_kepler_index[iter]])),lc_np_array_offset_mask[selected_kepler_index[iter]])\n",
    "\n",
    "#     # Interpolate the selected portion\n",
    "#     # Original data with 30 elements\n",
    "#     original_x = np.linspace(0, 100, num=len(selected_portion))\n",
    "#     original_y = selected_portion  # Replace with your actual data\n",
    "\n",
    "#     # Create a quadratic interpolation function\n",
    "#     f = interp1d(original_x, original_y, kind='quadratic')\n",
    "\n",
    "#     # Define the range of x-values for the interpolation with 120 elements\n",
    "#     x_interpolation = np.linspace(0, 100, num=len(train_lc_scaled_append[0]))\n",
    "\n",
    "#     # Perform the interpolation\n",
    "#     y_interpolated = f(x_interpolation)\n",
    "#     lc_np_array_interpol[iter] = y_interpolated\n",
    "#     plt.plot(np.linspace(0,1,len(lc_np_array_interpol[iter])),lc_np_array_interpol[iter],color='black')\n",
    "\n",
    "\n",
    "\n",
    "# plt.grid('on')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(ph_simul,train_lc_scaled_append[29],label='Simulated LC',linewidth=2,color='black')\n",
    "# kep_id_scale = int(np.random.choice(selected_kepler_index))\n",
    "\n",
    "# for iter in np.linspace(0,20,20,dtype=int):\n",
    "    \n",
    "#     start_index = (iter*5)\n",
    "#     stop_index = len(lc_np_array_offset[kep_id_scale]) - (iter*5)\n",
    "#     lc_scale_test = lc_np_array_offset[kep_id_scale][start_index:stop_index]\n",
    "#     ph_kepler_scale = np.linspace(-1,1,len(lc_scale_test))\n",
    "#     if iter == 0:\n",
    "#         plt.plot(ph_kepler_scale,lc_scale_test,label=f\"{kepid_array[kep_id_scale]} - Original\",color='tab:red',linewidth=3)\n",
    "#     elif iter%2==0:\n",
    "#         plt.plot(ph_kepler_scale,lc_scale_test,label=f\"Iter = {iter}\",color='tab:blue')\n",
    "# plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the number of \"1\"s to append on both sides\n",
    "# num_ones = 500\n",
    "\n",
    "# # Create arrays of \"1\"s for left and right sides\n",
    "# left_ones = np.ones(num_ones, dtype=int)\n",
    "# right_ones = np.ones(num_ones, dtype=int)\n",
    "\n",
    "# # Append \"1\"s to the left and right\n",
    "# lc_temp = train_lc_scaled_append[np.random.randint(len(train_lc_scaled_append))]\n",
    "# extended_lc_temp = np.concatenate((left_ones, lc_temp, right_ones))\n",
    "# extende_ph_simul = np.linspace(-1,1,len(extended_lc_temp))\n",
    "# plt.plot(ph_simul,lc_temp,label='Simulated LC',linewidth=2,color='red')\n",
    "# plt.plot(extende_ph_simul,extended_lc_temp,label='Extended Simulated LC',linewidth=2,color='black')\n",
    "# kep_id_scale = int(np.random.choice(selected_kepler_index))\n",
    "# lc_scale_test = lc_np_array_offset[kep_id_scale]\n",
    "# ph_kepler_scale = np.linspace(-1,1,len(lc_scale_test))\n",
    "# plt.plot(ph_kepler_scale,lc_scale_test,label=\"Kepler LC\",linewidth=2,color='tab:blue')\n",
    "# plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "# plt.xlim(-0.2,0.2)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load Dataset\n",
    "## Load Train Set\n",
    "# train_shape_dir = '../data/data_npy/shape_npy/shape_filled5.npy'\n",
    "# train_lc_dir = '../data/data_npy/lc_npy/lc_dict_5.npy'\n",
    "\n",
    "train_shape_dir = '../data/train/npy/shape/shape_5.npy'\n",
    "train_lc_dir =  '../data/train/npy/lc/lc_1_shape_5.npy'\n",
    "# train_lc_dir =  '../data/train/npy/lc/lc_2_shape_5.npy'\n",
    "train_lc = np.load(train_lc_dir)\n",
    "train_shape = np.load(train_shape_dir)\n",
    "# Check equality of number of dataset\n",
    "if len(train_lc)==len(train_shape):\n",
    "    print(\"Train Set: No. of LC = No. of shapes\")\n",
    "else:\n",
    "    sys.exit(\"EXIT: Train Set: No. of LC != No. of shapes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 5 x 5 shapes -= Train Set\n",
    "\n",
    "num = 5\n",
    "fig,ax=plt.subplots(num,5, figsize=(10,8), gridspec_kw={ 'width_ratios': [1,1,1,1,1],\n",
    "        'wspace': 0.5,'hspace': 0.25})\n",
    "\n",
    "iter = 0\n",
    "for i in range(num):\n",
    "    k = i # random.randint(3, len(x)-1)\n",
    "    rand_t = np.arange(1,6,1)\n",
    "    for j in range(5):\n",
    "        t = j + 100 # rand_t[j]\n",
    "        ax[i][j].tick_params(left = False, right = False , labelleft = False ,labelbottom = False, bottom = False)\n",
    "        # img = ax[i][j].imshow(y[k+t],cmap='inferno')\n",
    "        img = ax[i][j].imshow(train_shape[iter],cmap='gray')\n",
    "        plt.colorbar(img)\n",
    "        iter = iter + 1\n",
    "        # ax[i][1].imshow(y[k+1],cmap='inferno')\n",
    "        # ax[i][2].imshow(y[k+2],cmap='inferno')\n",
    "        # ax[i][3].imshow(y[k+3],cmap='inferno')\n",
    "        # ax[i][4].imshow(y[k+4],cmap='inferno')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
