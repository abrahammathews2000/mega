{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is used to overlap simulated light curve and \n",
    "# Kepler Lightcurve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TF and check for GPU\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Import required libraries\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from tensorflow.keras.models import save_model, load_model\n",
    "import math\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load kepler light curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = ['../data/tfr_shallue/tfr_all/tfrecord/train-00000-of-00008',\n",
    "             '../data/tfr_shallue/tfr_all/tfrecord/train-00001-of-00008',\n",
    "             '../data/tfr_shallue/tfr_all/tfrecord/train-00002-of-00008',\n",
    "             '../data/tfr_shallue/tfr_all/tfrecord/train-00003-of-00008',\n",
    "             '../data/tfr_shallue/tfr_all/tfrecord/train-00004-of-00008',\n",
    "             '../data/tfr_shallue/tfr_all/tfrecord/train-00005-of-00008',\n",
    "             '../data/tfr_shallue/tfr_all/tfrecord/train-00006-of-00008',\n",
    "             '../data/tfr_shallue/tfr_all/tfrecord/train-00007-of-00008',\n",
    "             '../data/tfr_shallue/tfr_all/tfrecord/val-00000-of-00001',\n",
    "             '../data/tfr_shallue/tfr_all/tfrecord/test-00000-of-00001']\n",
    "raw_dataset = tf.data.TFRecordDataset(filenames)\n",
    "raw_dataset\n",
    "\n",
    "feature_description = {\n",
    "    'global_view': tf.io.FixedLenSequenceFeature([], tf.float32, default_value=0.0,allow_missing=True),\n",
    "    'local_view': tf.io.FixedLenSequenceFeature([], tf.float32, default_value=0.0,allow_missing=True),\n",
    "    'av_training_set': tf.io.FixedLenFeature([], tf.string, default_value=''),\n",
    "    'kepid': tf.io.FixedLenSequenceFeature([], tf.int64, default_value=int(0),allow_missing=True),\n",
    "}\n",
    "\n",
    "def _parse_function(example_proto):\n",
    "  # Parse the input `tf.train.Example` proto using the dictionary above.\n",
    "  return tf.io.parse_single_example(example_proto, feature_description)\n",
    "\n",
    "# At this point the dataset contains serialized tf.train.Example messages.\n",
    "# When iterated over it returns these as scalar string tensors.\n",
    "# Use the .take method to only show the first 10 records.\n",
    "for raw_record in raw_dataset.take(10):\n",
    "  print(repr(raw_record))\n",
    "  \n",
    "parsed_dataset = raw_dataset.map(_parse_function)\n",
    "parsed_dataset\n",
    "\n",
    "\n",
    "# Calculate the size of one light curve corresponding to 'local_view'\n",
    "y = np.array([])\n",
    "for elem in parsed_dataset.take(1):\n",
    "  y = np.append(y,[elem['local_view']])\n",
    "length_lc = len(y)\n",
    "print(\"length_lc = \" ,length_lc)\n",
    "\n",
    "\n",
    "# Calculate total number of light curves\n",
    "no_data = 0\n",
    "for elem in parsed_dataset.as_numpy_iterator():\n",
    "    no_data = no_data + 1\n",
    "print('no_data = ',no_data)\n",
    "\n",
    "# Extract Kepler ID\n",
    "kepid_array = np.zeros(shape=(no_data,))\n",
    "i = 0\n",
    "for elem in parsed_dataset:\n",
    "    kepid_array[i] = elem['kepid'] \n",
    "    i = i + 1\n",
    "\n",
    "# convert all tf dataset to np array (light curve numpy array (lc_np_array))\n",
    "lc_np_array = np.zeros(shape=(no_data,length_lc))\n",
    "lc_np_array.shape\n",
    "\n",
    "i = 0\n",
    "for elem in parsed_dataset.as_numpy_iterator():\n",
    "    lc_np_array[i] = elem['local_view'] #+1.0\n",
    "    i = i + 1\n",
    "\n",
    "print('lc_np_array[0] = ',lc_np_array[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.arange(1):\n",
    "    j=981\n",
    "    test_lc = lc_np_array[j]\n",
    "    ph = np.linspace(-1.0,1.0,len(lc_np_array[0]))\n",
    "    plt.scatter(ph, test_lc)\n",
    "    print(int(kepid_array[j]))\n",
    "    plt.title(f\"{int(kepid_array[j])}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load simulation light curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load Dataset\n",
    "## Load Train Set\n",
    "train_shape_dir = '../data/data_npy/shape_npy/shape_filled5.npy'\n",
    "train_lc_dir = '../data/data_npy/lc_npy/lc_dict_5.npy'\n",
    "train_lc = np.load(train_lc_dir)\n",
    "train_shape = np.load(train_shape_dir)\n",
    "# Check equality of number of dataset\n",
    "if len(train_lc)==len(train_shape):\n",
    "    print(\"Train Set: No. of LC = No. of shapes\")\n",
    "else:\n",
    "    sys.exit(\"EXIT: Train Set: No. of LC != No. of shapes\")\n",
    "\n",
    "## Load Validation Set\n",
    "vald_shape_dir = '../data/data_npy/shape_npy/shape_filled4.npy'\n",
    "vald_lc_dir = '../data/data_npy/lc_npy/lc_dict_4.npy'\n",
    "vald_lc = np.load(vald_lc_dir)\n",
    "vald_shape = np.load(vald_shape_dir)\n",
    "# Check equality of nuftmber of dataset\n",
    "if len(vald_lc)==len(vald_shape):\n",
    "    print(\"Vald Set: No. of LC = No. of shapes\")\n",
    "else:\n",
    "    sys.exit(\"Vald Set: No. of LC = No. of shapes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Normalize the image, convert to opacity map\n",
    "## Train Set\n",
    "train_shape = train_shape/np.amax(train_shape)\n",
    "train_shape_where_0 = np.where(train_shape == 0)\n",
    "train_shape_where_1 = np.where(train_shape == 1)\n",
    "train_shape[train_shape_where_0] = 1  # 1 represent the shape (1 opacity)\n",
    "train_shape[train_shape_where_1] = 0  # 0 represent background (0 opacity)\n",
    "\n",
    "## Valdn Set\n",
    "vald_shape = vald_shape/np.amax(vald_shape)\n",
    "vald_shape_where_0 = np.where(vald_shape == 0)\n",
    "vald_shape_where_1 = np.where(vald_shape == 1)\n",
    "vald_shape[vald_shape_where_0] = 1  # 1 represent the shape (1 opacity)\n",
    "vald_shape[vald_shape_where_1] = 0  # 0 represent background (0 opacity)\n",
    "print(\"Normalized the shape\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Normalize the lightcurves\n",
    "## - Train Set\n",
    "train_lc_scaled = np.zeros(train_lc.shape)\n",
    "for i in np.arange(len(train_lc_scaled)):\n",
    "    train_lc_scaled[i] = (train_lc[i] - np.amin(train_lc[i]))/(np.amax(train_lc[i]) - np.amin(train_lc[i]))\n",
    "\n",
    "## - Vald Set\n",
    "vald_lc_scaled = np.zeros(vald_lc.shape)\n",
    "for i in np.arange(len(vald_lc_scaled)):\n",
    "    vald_lc_scaled[i] = (vald_lc[i] - np.amin(vald_lc[i]))/(np.amax(vald_lc[i]) - np.amin(vald_lc[i]))\n",
    "print(\"Normalized the light curves\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add flat line towards left and right of dip\n",
    "# 10 data points on each side\n",
    "# 3. Extend the lightcurves\n",
    "## - Train Set\n",
    "train_lc_scaled_append = np.ones((train_lc.shape[0],120))\n",
    "print('train_lc_scaled_append.shape = ',train_lc_scaled_append.shape)\n",
    "print(\"len(train_lc_scaled_append[0,10:110]) = \",len(train_lc_scaled_append[0,10:110]))\n",
    "\n",
    "for i in np.arange(len(train_lc_scaled)):\n",
    "    train_lc_scaled_append[i,10:110] = train_lc_scaled[i]\n",
    "\n",
    "## - Vald Set\n",
    "vald_lc_scaled_append = np.ones((vald_lc.shape[0],120))\n",
    "for i in np.arange(len(vald_lc_scaled)):\n",
    "    vald_lc_scaled_append[i,10:110] = vald_lc_scaled[i]\n",
    "print(\"Extended the light curves\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "# Plot - Train LCs\n",
    "num = 3\n",
    "fig,ax=plt.subplots(num,2, figsize=(4,3), gridspec_kw={ 'width_ratios': [2,1],\n",
    "        'wspace': 0.2,'hspace': 0.4})\n",
    "\n",
    "ax[0][1].set_title('Shape',size=15)\n",
    "ax[0][0].set_title('Light Curve (Train Dataset)',size=15)\n",
    "ax[num-1][0].set_xlabel('Phase',size=13)\n",
    "ph = np.linspace(-1,1,len(train_lc_scaled_append[0]))\n",
    "# advance = 60\n",
    "\n",
    "i = 0\n",
    "for i in np.arange(0,num):\n",
    "    k = np.random.randint(0, len(train_lc_scaled_append)-1)\n",
    "    ax[i][1].tick_params(left = False, right = False , labelleft = False ,labelbottom = False, bottom = False)\n",
    "    if(i<num-1): ax[i][0].tick_params(labelbottom = False, bottom = False)\n",
    "    img = ax[i][1].imshow(train_shape[k],cmap='inferno')\n",
    "    plt.colorbar(img)\n",
    "    ax[i][0].set_ylabel('Flux',size=13)\n",
    "    ax[i][0].set_ylim(-0.5,1.5)\n",
    "#     ax[i][0].scatter(ph, vald_lc_scaled_append[k],color = 'black',marker='.')\n",
    "    ax[i][0].plot(ph, train_lc_scaled_append[k],color = 'tab:red',linewidth='2')\n",
    "    ax[i][0].grid('on')\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "# Plot - vald LCs\n",
    "num = 3\n",
    "fig,ax=plt.subplots(num,2, figsize=(4,3), gridspec_kw={ 'width_ratios': [2,1],\n",
    "        'wspace': 0.2,'hspace': 0.4})\n",
    "\n",
    "ax[0][1].set_title('Shape',size=15)\n",
    "ax[0][0].set_title('Light Curve (vald Dataset)',size=15)\n",
    "ax[num-1][0].set_xlabel('Phase',size=13)\n",
    "ph = np.linspace(-1,1,len(vald_lc_scaled_append[0]))\n",
    "# advance = 60\n",
    "\n",
    "i = 0\n",
    "for i in np.arange(0,num):\n",
    "    k = np.random.randint(0, len(vald_lc_scaled_append)-1)\n",
    "    ax[i][1].tick_params(left = False, right = False , labelleft = False ,labelbottom = False, bottom = False)\n",
    "    if(i<num-1): ax[i][0].tick_params(labelbottom = False, bottom = False)\n",
    "    img = ax[i][1].imshow(vald_shape[k],cmap='inferno')\n",
    "    plt.colorbar(img)\n",
    "    ax[i][0].set_ylabel('Flux',size=13)\n",
    "    ax[i][0].set_ylim(-0.5,1.5)\n",
    "#     ax[i][0].scatter(ph, vald_lc_scaled_append[k],color = 'black',marker='.')\n",
    "    ax[i][0].plot(ph, vald_lc_scaled_append[k],color = 'tab:red',linewidth='2')\n",
    "    ax[i][0].grid('on')\n",
    "    i = i + 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
