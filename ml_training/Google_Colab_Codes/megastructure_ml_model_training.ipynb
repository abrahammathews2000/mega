{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ApEgl_ktfKVR",
        "outputId": "10d1272f-afd1-4206-f3e7-4200e550816d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# .py code help to run the code in screen mode .ipynb cannot do that\n",
        "# This code can be used to load light curve training dataset and do the\n",
        "# preprocessing like vertical scaling and extending the light curve\n",
        "# CNN Model is defined\n",
        "# Training, learning rate scheduler and early stopping feature included\n",
        "\n",
        "## -- IMP: Check whether the file name to save the model is complete ##\n",
        "\n",
        "# Import TF and check for GPU\n",
        "\n",
        "import tensorflow as tf\n",
        "print(tf.config.list_physical_devices('GPU'))\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "\n",
        "# Import required libraries\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from tensorflow.keras.models import save_model, load_model\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split\n",
        "import sys\n",
        "from numpy import array,append,arange,zeros,exp,sin,random,std\n",
        "from scipy.interpolate import interp1d\n",
        "\n",
        "import time\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-9DuJdLsNFn",
        "outputId": "b91d971a-6cf3-4202-e2e3-3737f07aac68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "TensorFlow version: 2.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "knC952c7aw7o",
        "outputId": "2f08a113-7b63-4bb3-b4ee-3a3b2bdeaf44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_shape_dir = /content/drive/MyDrive/5_AstroFYP_data/april_25_model_training/train_shape_5_2times.npy\n",
            "train_lc_dir = /content/drive/MyDrive/5_AstroFYP_data/april_25_model_training/concat_lc_10_12_shape_5_multisize_multiLDC.npy\n",
            "train_lc_org.shape = (264272, 100)\n",
            "train_shape.shape =  (264272, 38, 38)\n",
            "Train Set: No. of LC = No. of shapes\n",
            "vald_shape_dir = /content/drive/MyDrive/5_AstroFYP_data/april_25_model_training/vald_shape_1.npy\n",
            "vald_lc_dir = /content/drive/MyDrive/5_AstroFYP_data/april_25_model_training/vald_lc_10_shape_multisize_multiLDC.npy\n",
            "vald_lc.shape = (1000, 100)\n",
            "vald_shape.shape =  (1000, 38, 38)\n",
            "Vald Set: No. of LC = No. of shapes\n",
            "Normalized the shape\n"
          ]
        }
      ],
      "source": [
        "## Edit the base folder to save model, weights and plots\n",
        "base_model_folder = '/content/drive/MyDrive/5_AstroFYP_data/april_25_model_training/ml_model/'\n",
        "\n",
        "\n",
        "# 1. Load Dataset\n",
        "## Load Train Set\n",
        "train_shape_dir = '/content/drive/MyDrive/5_AstroFYP_data/april_25_model_training/train_shape_5_2times.npy'\n",
        "train_lc_dir = '/content/drive/MyDrive/5_AstroFYP_data/april_25_model_training/concat_lc_10_12_shape_5_multisize_multiLDC.npy'\n",
        "\n",
        "print(f\"train_shape_dir = {train_shape_dir}\")\n",
        "print(f\"train_lc_dir = {train_lc_dir}\")\n",
        "\n",
        "train_lc_org = np.load(train_lc_dir)\n",
        "train_shape = np.load(train_shape_dir)\n",
        "# Check equality of number of dataset\n",
        "print('train_lc_org.shape =',train_lc_org.shape)\n",
        "print('train_shape.shape = ',train_shape.shape)\n",
        "if len(train_lc_org)==len(train_shape):\n",
        "    print(\"Train Set: No. of LC = No. of shapes\")\n",
        "else:\n",
        "    sys.exit(\"EXIT: Train Set: No. of LC != No. of shapes\")\n",
        "\n",
        "## Load Validation Set\n",
        "vald_shape_dir = '/content/drive/MyDrive/5_AstroFYP_data/april_25_model_training/vald_shape_1.npy'\n",
        "vald_lc_dir = '/content/drive/MyDrive/5_AstroFYP_data/april_25_model_training/vald_lc_10_shape_multisize_multiLDC.npy'\n",
        "\n",
        "print(f\"vald_shape_dir = {vald_shape_dir}\")\n",
        "print(f\"vald_lc_dir = {vald_lc_dir}\")\n",
        "\n",
        "vald_lc_org = np.load(vald_lc_dir)\n",
        "vald_shape = np.load(vald_shape_dir)\n",
        "# Check equality of nuftmber of dataset\n",
        "print('vald_lc.shape =',vald_lc_org.shape)\n",
        "print('vald_shape.shape = ',vald_shape.shape)\n",
        "if len(vald_lc_org)==len(vald_shape):\n",
        "    print(\"Vald Set: No. of LC = No. of shapes\")\n",
        "else:\n",
        "    sys.exit(\"Vald Set: No. of LC = No. of shapes\")\n",
        "\n",
        "# 2. Normalize the image, convert to opacity map\n",
        "## Train Set\n",
        "train_shape = train_shape/np.amax(train_shape)\n",
        "\n",
        "train_shape[np.where(train_shape == 0)] = 5  # 1 represent the shape (1 opacity)\n",
        "train_shape[np.where(train_shape == 1)] = 0  # 0 represent background (0 opacity)\n",
        "train_shape[np.where(train_shape == 5)] = 1\n",
        "\n",
        "## Valdn Set\n",
        "vald_shape = vald_shape/np.amax(vald_shape)\n",
        "\n",
        "vald_shape[np.where(vald_shape == 0)] = 5  # 1 represent the shape (1 opacity)\n",
        "vald_shape[np.where(vald_shape == 1)] = 0  # 0 represent background (0 opacity)\n",
        "vald_shape[np.where(vald_shape == 5)] = 1\n",
        "\n",
        "\n",
        "print(\"Normalized the shape\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_lc = train_lc_org\n",
        "vald_lc = vald_lc_org\n",
        "\n",
        "# 3. Vertically Scaling\n",
        "## - Train Set\n",
        "\n",
        "for i in np.arange(len(train_lc)):\n",
        "    train_lc[i] = (train_lc[i] - np.amin(train_lc[i]))/(np.amax(train_lc[i]) - np.amin(train_lc[i]))\n",
        "\n",
        "## - Vald Set\n",
        "\n",
        "for i in np.arange(len(vald_lc)):\n",
        "    vald_lc[i] = (vald_lc[i] - np.amin(vald_lc[i]))/(np.amax(vald_lc[i]) - np.amin(vald_lc[i]))\n",
        "print(\"Vertically Scaled the light curves\")\n",
        "\n",
        "\n",
        "# Add flat line towards left and right of dip\n",
        "# 10 data points on each side\n",
        "# 4. Extend the lightcurves\n",
        "## - Train Set\n",
        "train_lc_scaled_append = np.ones((train_lc.shape[0],150))\n",
        "for i in np.arange(len(train_lc)):\n",
        "    train_lc_scaled_append[i,25:125] = train_lc[i]\n",
        "\n",
        "del train_lc\n",
        "train_lc = train_lc_scaled_append\n",
        "del train_lc_scaled_append\n",
        "\n",
        "## - Vald Set\n",
        "vald_lc_scaled_append = np.ones((vald_lc.shape[0],150))\n",
        "for i in np.arange(len(vald_lc)):\n",
        "    vald_lc_scaled_append[i,25:125] = vald_lc[i]\n",
        "\n",
        "del vald_lc\n",
        "vald_lc = vald_lc_scaled_append\n",
        "del vald_lc_scaled_append\n",
        "print(\"Extended the light curves\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-QWij9buLrR",
        "outputId": "61b5f7a3-d99e-4cfd-d9d8-7c9d777c783c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vertically Scaled the light curves\n",
            "Extended the light curves\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Horizontal scaling\n",
        "def scale_horizontally(input_lc_dataset):\n",
        "    # lc_np_array_offset_mask used to select the flat part by certain percentage\n",
        "    input_lc_dataset_mask = np.copy(input_lc_dataset)\n",
        "\n",
        "    for iteration in np.arange(len(input_lc_dataset)):\n",
        "        # 0.988 is working good | lower it and see changes # 0.96 - 0.97 -better # 0.95 -worse\n",
        "        input_lc_dataset_mask[iteration][(input_lc_dataset[iteration]>=0.98)] = 1.0\n",
        "        input_lc_dataset_mask[iteration][(input_lc_dataset[iteration]<0.98)] = 0.0\n",
        "\n",
        "    print(\"Length of one LC = \", len(input_lc_dataset_mask[0]))\n",
        "\n",
        "    count_zeros_array = np.zeros((len(input_lc_dataset_mask),))\n",
        "    for iteration in np.arange(len(input_lc_dataset_mask)):\n",
        "        # Calculate the number of occurrences of '0'\n",
        "        count_zeros = np.count_nonzero(input_lc_dataset_mask[iteration] == 0)\n",
        "        count_zeros_array[iteration] = count_zeros\n",
        "\n",
        "    # Interpolate the light curve\n",
        "    input_lc_dataset_interpol = np.zeros((len(input_lc_dataset), 120))\n",
        "    len_selected_portion = np.zeros(len(input_lc_dataset))\n",
        "    print(\"input_lc_dataset_interpol.shape =\", input_lc_dataset_interpol.shape)\n",
        "\n",
        "    center_index = int(len(input_lc_dataset[0])/2)\n",
        "    print(\"center_index =\", center_index)\n",
        "\n",
        "    for iteration in np.arange(len(input_lc_dataset_interpol)):\n",
        "\n",
        "        left_index = int(center_index - int(count_zeros_array[iteration]/2) - int(count_zeros_array[iteration]/6))\n",
        "        right_index = int(center_index + int(count_zeros_array[iteration]/2) + int(count_zeros_array[iteration]/6))\n",
        "        selected_portion = input_lc_dataset[iteration][left_index:right_index]\n",
        "        # print(\"left_index =\", left_index)\n",
        "        # print(\"right_index =\", right_index)\n",
        "\n",
        "        # Calculate the length of the selected region\n",
        "        len_selected_portion[iteration] = len(selected_portion)\n",
        "\n",
        "        # Interpolate the selected portion\n",
        "        # Original data\n",
        "        original_x = np.linspace(-1, 1, num=len(selected_portion))\n",
        "        original_y = selected_portion\n",
        "\n",
        "        # Create a quadratic interpolation function\n",
        "        f = interp1d(original_x, original_y, kind='quadratic')\n",
        "\n",
        "        # Define the range of x-values for the interpolation with 120 elements\n",
        "        x_interpolation = np.linspace(-1, 1, num=120)\n",
        "\n",
        "        # Perform the interpolation\n",
        "        y_interpolated = f(x_interpolation)\n",
        "        input_lc_dataset_interpol[iteration] = y_interpolated\n",
        "\n",
        "    return input_lc_dataset_interpol\n",
        "\n",
        "train_lc = scale_horizontally(train_lc)\n",
        "vald_lc = scale_horizontally(vald_lc)\n",
        "\n",
        "print(\"train_lc.shape = \", train_lc.shape)\n",
        "print(\"vald_lc.shape = \", vald_lc.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pU_rjQzqoocK",
        "outputId": "9100bd28-5217-4aa0-cece-07d15b2d3b6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of one LC =  150\n",
            "input_lc_dataset_interpol.shape = (264272, 120)\n",
            "center_index = 75\n",
            "Length of one LC =  150\n",
            "input_lc_dataset_interpol.shape = (1000, 120)\n",
            "center_index = 75\n",
            "train_lc.shape =  (264272, 120)\n",
            "vald_lc.shape =  (1000, 120)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verification\n",
        "# Plot - Train LCs\n",
        "plt.clf()\n",
        "num = 3\n",
        "fig,ax=plt.subplots(num,\n",
        "                    2,\n",
        "                    figsize=(6,5),\n",
        "                    gridspec_kw={'width_ratios': [2,1], 'wspace': 0.2, 'hspace': 0.4}\n",
        ")\n",
        "\n",
        "ax[0][1].set_title('Shape',size=15)\n",
        "ax[0][0].set_title('Light Curve (Train Dataset)',size=15)\n",
        "ax[num-1][0].set_xlabel('Phase',size=13)\n",
        "ph = np.linspace(-1,1,len(train_lc[0]))\n",
        "\n",
        "i = 0\n",
        "for i in np.arange(0, num):\n",
        "    k = np.random.randint(0, len(train_lc)-1)\n",
        "    ax[i][1].tick_params(left=False,\n",
        "                         right=False,\n",
        "                         labelleft=False,\n",
        "                         labelbottom=False,\n",
        "                         bottom=False\n",
        "    )\n",
        "    if(i<num-1): ax[i][0].tick_params(labelbottom = False, bottom = False)\n",
        "    img = ax[i][1].imshow(train_shape[k], cmap='inferno')\n",
        "    plt.colorbar(img)\n",
        "    ax[i][0].set_ylabel('Flux', size=13)\n",
        "    # ax[i][0].set_ylim(-0.5, 1.5)\n",
        "    ax[i][0].plot(ph, train_lc[k], color = 'tab:red', linewidth='2')\n",
        "    ax[i][0].grid('on')\n",
        "    i = i + 1\n",
        "plt.savefig(base_model_folder+'plot_train_lc.png')\n",
        "plt.close()\n",
        "\n",
        "# Plot - Vald LCs\n",
        "plt.clf()\n",
        "num = 3\n",
        "fig,ax=plt.subplots(num,\n",
        "                    2,\n",
        "                    figsize=(4, 3),\n",
        "                    gridspec_kw={'width_ratios':[2,1], 'wspace':0.2, 'hspace':0.4})\n",
        "\n",
        "ax[0][1].set_title('Shape',size=15)\n",
        "ax[0][0].set_title('Light Curve (Vald Dataset)',size=15)\n",
        "ax[num-1][0].set_xlabel('Phase',size=13)\n",
        "ph = np.linspace(-1,1,len(vald_lc[0]))\n",
        "\n",
        "i = 0\n",
        "for i in np.arange(0,num):\n",
        "    k = np.random.randint(0, len(vald_lc)-1)\n",
        "    ax[i][1].tick_params(left=False,\n",
        "                         right=False,\n",
        "                         labelleft=False,\n",
        "                         labelbottom=False,\n",
        "                         bottom=False\n",
        "    )\n",
        "    if(i<num-1): ax[i][0].tick_params(labelbottom = False, bottom = False)\n",
        "    img = ax[i][1].imshow(vald_shape[k],cmap='inferno')\n",
        "    plt.colorbar(img)\n",
        "    ax[i][0].set_ylabel('Flux',size=13)\n",
        "    # ax[i][0].set_ylim(-0.5,1.5)\n",
        "    # ax[i][0].scatter(ph, vald_lc_scaled_append[k],color = 'black',marker='.')\n",
        "    ax[i][0].plot(ph, vald_lc[k], color = 'tab:red', linewidth='2')\n",
        "    ax[i][0].grid('on')\n",
        "    i = i + 1\n",
        "plt.savefig(base_model_folder+'plot_vald_lc.png')\n",
        "plt.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "NfY0iY33t_0h",
        "outputId": "69afa5f5-fbb5-41c7-9349-8f924dd3c01a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## START - Add noise\n",
        "random_generator = np.random.default_rng()\n",
        "\n",
        "SNR_array_train = random_generator.uniform(50, 500, len(train_lc))\n",
        "std_dev_train = 1/SNR_array_train\n",
        "del SNR_array_train\n",
        "\n",
        "SNR_array_vald = random_generator.uniform(50, 500, len(vald_lc))\n",
        "std_dev_vald = 1/SNR_array_vald\n",
        "del SNR_array_vald\n",
        "\n",
        "for i in np.arange(len(train_lc)):\n",
        "    train_lc[i] = train_lc[i] + np.random.normal(loc=0.0, scale=std_dev_train[i], size=len(train_lc[i]))\n",
        "del std_dev_train\n",
        "\n",
        "for i in np.arange(len(vald_lc)):\n",
        "    vald_lc[i] = vald_lc[i] + np.random.normal(loc=0.0, scale=std_dev_vald[i], size=len(vald_lc[i]))\n",
        "del std_dev_vald\n",
        "## END - Add noise\n",
        "print(\"After adding noise\")\n",
        "\n",
        "print(f\"train_lc.shape = {train_lc.shape}\")\n",
        "print(f\"vald_lc.shape = {vald_lc.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnL4U7YXszCg",
        "outputId": "b57f6e26-ac03-4771-99fa-f0f1b5a2e915"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After adding noise\n",
            "train_lc.shape = (264272, 120)\n",
            "vald_lc.shape = (1000, 120)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verification\n",
        "# Plot - Train LCs\n",
        "plt.clf()\n",
        "num = 3\n",
        "fig,ax=plt.subplots(num,\n",
        "                    2,\n",
        "                    figsize=(6,5),\n",
        "                    gridspec_kw={'width_ratios': [2,1], 'wspace': 0.2, 'hspace': 0.4}\n",
        ")\n",
        "\n",
        "ax[0][1].set_title('Shape',size=15)\n",
        "ax[0][0].set_title('Light Curve (Train Dataset)',size=15)\n",
        "ax[num-1][0].set_xlabel('Phase',size=13)\n",
        "ph = np.linspace(-1,1,len(train_lc[0]))\n",
        "\n",
        "i = 0\n",
        "for i in np.arange(0, num):\n",
        "    k = np.random.randint(0, len(train_lc)-1)\n",
        "    ax[i][1].tick_params(left=False,\n",
        "                         right=False,\n",
        "                         labelleft=False,\n",
        "                         labelbottom=False,\n",
        "                         bottom=False\n",
        "    )\n",
        "    if(i<num-1): ax[i][0].tick_params(labelbottom = False, bottom = False)\n",
        "    img = ax[i][1].imshow(train_shape[k], cmap='inferno')\n",
        "    plt.colorbar(img)\n",
        "    ax[i][0].set_ylabel('Flux', size=13)\n",
        "    # ax[i][0].set_ylim(-0.5, 1.5)\n",
        "    ax[i][0].plot(ph, train_lc[k], color = 'tab:red', linewidth='2')\n",
        "    ax[i][0].grid('on')\n",
        "    i = i + 1\n",
        "plt.savefig(base_model_folder+'plot_train_lc_noise.png')\n",
        "plt.close()\n",
        "\n",
        "# Plot - Vald LCs\n",
        "plt.clf()\n",
        "num = 3\n",
        "fig,ax=plt.subplots(num,\n",
        "                    2,\n",
        "                    figsize=(4, 3),\n",
        "                    gridspec_kw={'width_ratios':[2,1], 'wspace':0.2, 'hspace':0.4})\n",
        "\n",
        "ax[0][1].set_title('Shape',size=15)\n",
        "ax[0][0].set_title('Light Curve (Vald Dataset)',size=15)\n",
        "ax[num-1][0].set_xlabel('Phase',size=13)\n",
        "ph = np.linspace(-1,1,len(vald_lc[0]))\n",
        "\n",
        "i = 0\n",
        "for i in np.arange(0,num):\n",
        "    k = np.random.randint(0, len(vald_lc)-1)\n",
        "    ax[i][1].tick_params(left=False,\n",
        "                         right=False,\n",
        "                         labelleft=False,\n",
        "                         labelbottom=False,\n",
        "                         bottom=False\n",
        "    )\n",
        "    if(i<num-1): ax[i][0].tick_params(labelbottom = False, bottom = False)\n",
        "    img = ax[i][1].imshow(vald_shape[k],cmap='inferno')\n",
        "    plt.colorbar(img)\n",
        "    ax[i][0].set_ylabel('Flux',size=13)\n",
        "    # ax[i][0].set_ylim(-0.5,1.5)\n",
        "    # ax[i][0].scatter(ph, vald_lc_scaled_append[k],color = 'black',marker='.')\n",
        "    ax[i][0].plot(ph, vald_lc[k], color = 'tab:red', linewidth='2')\n",
        "    ax[i][0].grid('on')\n",
        "    i = i + 1\n",
        "plt.savefig(base_model_folder+'plot_vald_lc_noise.png')\n",
        "plt.close()\n",
        "\n",
        "\n",
        "# Model path to be saved to\n",
        "model_save_path = base_model_folder+\"april24_2024_model_unfDist_LDC_size_horz_scale.h5\"\n",
        "print(f\"model_save_path = {model_save_path}\")\n",
        "\n",
        "no_epochs = int(200) # For testing start with small value of 3 or 5\n",
        "print(\"no_epochs =\",no_epochs)\n",
        "\n",
        "# user_input = input(\"Do you want to run the code? (y/n): \")\n",
        "# if user_input.lower() != \"y\":\n",
        "#     sys.exit(\"EXIT: User declined to continue the program\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "PcZ16txKgJFQ",
        "outputId": "b4ea5550-110f-4fb8-a35c-8a2c820a0412"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_save_path = /content/drive/MyDrive/5_AstroFYP_data/april_25_model_training/ml_model/april24_2024_model_unfDist_LDC_size_horz_scale.h5\n",
            "no_epochs = 200\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tic = time.perf_counter()\n",
        "print(\"Creating ML Pipeline\")\n",
        "# 6. ML Pipeline\n",
        "## Train Set\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_lc, train_shape))\n",
        "train_dataset = train_dataset.cache()\n",
        "train_dataset = train_dataset.shuffle(len(train_dataset))\n",
        "train_dataset = train_dataset.batch(100)\n",
        "train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
        "print(train_dataset)\n",
        "\n",
        "## Vald Set\n",
        "vald_dataset = tf.data.Dataset.from_tensor_slices((vald_lc, vald_shape))\n",
        "vald_dataset = vald_dataset.batch(100)\n",
        "vald_dataset = vald_dataset.cache()\n",
        "vald_dataset = vald_dataset.prefetch(tf.data.AUTOTUNE)\n",
        "print(vald_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6gLczYRqV-P",
        "outputId": "dec7e831-e996-44a0-87c2-25836b54abc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating ML Pipeline\n",
            "<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 120), dtype=tf.float64, name=None), TensorSpec(shape=(None, 38, 38), dtype=tf.float64, name=None))>\n",
            "<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 120), dtype=tf.float64, name=None), TensorSpec(shape=(None, 38, 38), dtype=tf.float64, name=None))>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN Model\n",
        "input_shape = np.array(np.shape(train_lc[0]))\n",
        "print(\"np.shape(input_shape) =\", input_shape[0])\n",
        "\n",
        "output_shape = np.array(np.shape(train_shape[0]))\n",
        "print(\"np.shape(input_shape) =\", output_shape[0], output_shape[1])\n",
        "\n",
        "START = input_shape[0]\n",
        "END = output_shape[0]\n",
        "print(\"Start =\", START)\n",
        "print(\"End =\", END)\n",
        "\n",
        "conv_ip = keras.layers.Input(shape=(START,), name='Input')\n",
        "x= keras.layers.Reshape((START, 1), input_shape=(START,), name='reshape_1')(conv_ip)\n",
        "x= keras.layers.BatchNormalization()(x)\n",
        "\n",
        "x=keras.layers.Conv1D(16,\n",
        "                      kernel_size=5,\n",
        "                      strides=1,\n",
        "                      activation='relu',\n",
        "                      name='conv16_5',\n",
        "                      padding='same')(x)\n",
        "\n",
        "x=keras.layers.Conv1D(16,\n",
        "                      kernel_size=5,\n",
        "                      strides=1,\n",
        "                      activation='relu',\n",
        "                      name='second_conv16_5',\n",
        "                      padding='same')(x)\n",
        "\n",
        "x=keras.layers.MaxPool1D(5,\n",
        "                         strides=2,\n",
        "                         data_format='channels_last',\n",
        "                         name='maxpool_1',\n",
        "                         padding='same')(x)\n",
        "\n",
        "x=keras.layers.Conv1D(32,\n",
        "                      kernel_size=5,\n",
        "                      strides=1,\n",
        "                      activation='relu',\n",
        "                      name='first_conv32_5',\n",
        "                      padding='same')(x)\n",
        "\n",
        "x=keras.layers.Conv1D(32,\n",
        "                      kernel_size=5,\n",
        "                      strides=1,\n",
        "                      activation='relu',\n",
        "                      name='second_conv32_5',\n",
        "                      padding='same')(x)\n",
        "\n",
        "x=keras.layers.MaxPool1D(5,\n",
        "                         strides=2,\n",
        "                         data_format='channels_last',\n",
        "                         name='maxpool_2',\n",
        "                         padding='same')(x) #200\n",
        "\n",
        "x=keras.layers.Conv1D(64,\n",
        "                      kernel_size=5,\n",
        "                      strides=1,\n",
        "                      activation='relu',\n",
        "                      name='first_conv64_5',\n",
        "                      padding='same')(x)\n",
        "\n",
        "x=keras.layers.Conv1D(64,\n",
        "                      kernel_size=5,\n",
        "                      strides=1,\n",
        "                      activation='relu',\n",
        "                      name='second_conv64_5',\n",
        "                      padding='same')(x)\n",
        "\n",
        "x=keras.layers.MaxPool1D(5,\n",
        "                         strides=2,\n",
        "                         data_format='channels_last',\n",
        "                         name='maxpool_3',\n",
        "                         padding='same')(x)\n",
        "\n",
        "x=keras.layers.Flatten(name='flat_1')(x)\n",
        "\n",
        "x=keras.layers.Dense(256, name='dense_layer_5', activation='relu')(x)\n",
        "x=keras.layers.Dense(256, name='dense_layer_6', activation='relu')(x)\n",
        "\n",
        "x= keras.layers.Dense(END**2, name='dense_layer_u', activation='relu')(x)\n",
        "x = keras.layers.Reshape(target_shape=(END, END, 1), name='reshape_2')(x)\n",
        "\n",
        "x=keras.layers.Conv2D(32,\n",
        "                       kernel_size=(3,3),\n",
        "                       strides=1,\n",
        "                       activation='relu',\n",
        "                       name='second_conv64_52',\n",
        "                       padding='same')(x)\n",
        "\n",
        "x=keras.layers.Conv2D(32,\n",
        "                       kernel_size=(3,3),\n",
        "                       strides=1,\n",
        "                       activation='relu',\n",
        "                       name='second_conv64_522',\n",
        "                       padding='same')(x)\n",
        "\n",
        "x=keras.layers.Conv2D(16,\n",
        "                       kernel_size=(3,3),\n",
        "                       strides=1,\n",
        "                       activation='relu',\n",
        "                       name='second_conv64_524',\n",
        "                       padding='same')(x)\n",
        "\n",
        "x=keras.layers.Conv2D(1,\n",
        "                       kernel_size=3,\n",
        "                       strides=1,\n",
        "                       activation='relu',\n",
        "                       name='second_conv64_53',\n",
        "                       padding='same')(x)\n",
        "\n",
        "conv_op = keras.layers.Reshape(target_shape=(END, END), name='reshape_3')(x)\n",
        "\n",
        "model = keras.Model(inputs=conv_ip, outputs=conv_op, name=\"predict_shape_from_LC\")\n",
        "model.summary()\n",
        "\n",
        "print(\"Model is defined\")\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001), loss='mse')\n",
        "print(\"Model is compiled\")\n",
        "\n",
        "# Patience early stopping\n",
        "es = keras.callbacks.EarlyStopping(monitor='val_loss',\n",
        "                                   mode='min',\n",
        "                                   verbose=1,\n",
        "                                   patience=30\n",
        ")\n",
        "print(\"Early stopping defined\")\n",
        "\n",
        "# Learning rate scheduler\n",
        "def step_decay(epoch):\n",
        "\tinitial_lrate = 0.001\n",
        "\tdrop = 0.5\n",
        "\tepochs_drop = 20\n",
        "\tlrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
        "\treturn lrate\n",
        "lr_sched = keras.callbacks.LearningRateScheduler(step_decay)\n",
        "print(\"Learning rate scheduler defined\")\n",
        "\n",
        "# Model checkpoint\n",
        "checkpoint_path = base_model_folder+\"ckpt/checkpoint.weights.h5\"\n",
        "\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    checkpoint_path,\n",
        "    monitor='val_loss',\n",
        "    verbose=0,\n",
        "    save_best_only=False,\n",
        "    save_weights_only=True,\n",
        "    mode='min',\n",
        "    save_freq='epoch',\n",
        "    initial_value_threshold=None\n",
        ")\n",
        "print(\"Model checkpoint defined\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMkSJXrPqPoS",
        "outputId": "5b51af66-ff2e-41cd-9924-3ef9a8dc7497"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "np.shape(input_shape) = 120\n",
            "np.shape(input_shape) = 38 38\n",
            "Start = 120\n",
            "End = 38\n",
            "Model: \"predict_shape_from_LC\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " Input (InputLayer)          [(None, 120)]             0         \n",
            "                                                                 \n",
            " reshape_1 (Reshape)         (None, 120, 1)            0         \n",
            "                                                                 \n",
            " batch_normalization (Batch  (None, 120, 1)            4         \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " conv16_5 (Conv1D)           (None, 120, 16)           96        \n",
            "                                                                 \n",
            " second_conv16_5 (Conv1D)    (None, 120, 16)           1296      \n",
            "                                                                 \n",
            " maxpool_1 (MaxPooling1D)    (None, 60, 16)            0         \n",
            "                                                                 \n",
            " first_conv32_5 (Conv1D)     (None, 60, 32)            2592      \n",
            "                                                                 \n",
            " second_conv32_5 (Conv1D)    (None, 60, 32)            5152      \n",
            "                                                                 \n",
            " maxpool_2 (MaxPooling1D)    (None, 30, 32)            0         \n",
            "                                                                 \n",
            " first_conv64_5 (Conv1D)     (None, 30, 64)            10304     \n",
            "                                                                 \n",
            " second_conv64_5 (Conv1D)    (None, 30, 64)            20544     \n",
            "                                                                 \n",
            " maxpool_3 (MaxPooling1D)    (None, 15, 64)            0         \n",
            "                                                                 \n",
            " flat_1 (Flatten)            (None, 960)               0         \n",
            "                                                                 \n",
            " dense_layer_5 (Dense)       (None, 256)               246016    \n",
            "                                                                 \n",
            " dense_layer_6 (Dense)       (None, 256)               65792     \n",
            "                                                                 \n",
            " dense_layer_u (Dense)       (None, 1444)              371108    \n",
            "                                                                 \n",
            " reshape_2 (Reshape)         (None, 38, 38, 1)         0         \n",
            "                                                                 \n",
            " second_conv64_52 (Conv2D)   (None, 38, 38, 32)        320       \n",
            "                                                                 \n",
            " second_conv64_522 (Conv2D)  (None, 38, 38, 32)        9248      \n",
            "                                                                 \n",
            " second_conv64_524 (Conv2D)  (None, 38, 38, 16)        4624      \n",
            "                                                                 \n",
            " second_conv64_53 (Conv2D)   (None, 38, 38, 1)         145       \n",
            "                                                                 \n",
            " reshape_3 (Reshape)         (None, 38, 38)            0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 737241 (2.81 MB)\n",
            "Trainable params: 737239 (2.81 MB)\n",
            "Non-trainable params: 2 (8.00 Byte)\n",
            "_________________________________________________________________\n",
            "Model is defined\n",
            "Model is compiled\n",
            "Early stopping defined\n",
            "Learning rate scheduler defined\n",
            "Model checkpoint defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "print(\"Training will start now\")\n",
        "\n",
        "history = model.fit(train_dataset,\n",
        "                    epochs=no_epochs,\n",
        "                    verbose=2,\n",
        "                    validation_data=vald_dataset,\n",
        "                    callbacks=[es, lr_sched, model_checkpoint_callback]\n",
        ")\n",
        "\n",
        "# Save Model\n",
        "save_model(model, str(model_save_path))\n",
        "print(\"Model saved\")\n",
        "\n",
        "plt.plot(history.history['loss'], label=\"Training Loss\")\n",
        "plt.plot(history.history['val_loss'], label=\"Validation Loss\")\n",
        "plt.legend()\n",
        "plt.savefig(base_model_folder+'history_loss_graph.png')\n",
        "plt.close()\n",
        "\n",
        "toc = time.perf_counter()\n",
        "print(toc-tic, \" s\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jq1EwNRU2ADi",
        "outputId": "b74465a1-e2f6-490c-8e79-acabd788acbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training will start now\n",
            "Epoch 1/200\n",
            "2643/2643 - 44s - loss: 0.1578 - val_loss: 0.1494 - lr: 0.0010 - 44s/epoch - 17ms/step\n",
            "Epoch 2/200\n",
            "2643/2643 - 32s - loss: 0.1491 - val_loss: 0.1485 - lr: 0.0010 - 32s/epoch - 12ms/step\n",
            "Epoch 3/200\n",
            "2643/2643 - 32s - loss: 0.1481 - val_loss: 0.1472 - lr: 0.0010 - 32s/epoch - 12ms/step\n",
            "Epoch 4/200\n",
            "2643/2643 - 31s - loss: 0.1475 - val_loss: 0.1463 - lr: 0.0010 - 31s/epoch - 12ms/step\n",
            "Epoch 5/200\n",
            "2643/2643 - 31s - loss: 0.1470 - val_loss: 0.1466 - lr: 0.0010 - 31s/epoch - 12ms/step\n",
            "Epoch 6/200\n",
            "2643/2643 - 31s - loss: 0.1466 - val_loss: 0.1456 - lr: 0.0010 - 31s/epoch - 12ms/step\n",
            "Epoch 7/200\n",
            "2643/2643 - 31s - loss: 0.1462 - val_loss: 0.1457 - lr: 0.0010 - 31s/epoch - 12ms/step\n",
            "Epoch 8/200\n",
            "2643/2643 - 31s - loss: 0.1459 - val_loss: 0.1450 - lr: 0.0010 - 31s/epoch - 12ms/step\n",
            "Epoch 9/200\n",
            "2643/2643 - 31s - loss: 0.1456 - val_loss: 0.1453 - lr: 0.0010 - 31s/epoch - 12ms/step\n",
            "Epoch 10/200\n",
            "2643/2643 - 31s - loss: 0.1454 - val_loss: 0.1452 - lr: 0.0010 - 31s/epoch - 12ms/step\n",
            "Epoch 11/200\n",
            "2643/2643 - 31s - loss: 0.1452 - val_loss: 0.1439 - lr: 0.0010 - 31s/epoch - 12ms/step\n",
            "Epoch 12/200\n",
            "2643/2643 - 31s - loss: 0.1450 - val_loss: 0.1445 - lr: 0.0010 - 31s/epoch - 12ms/step\n",
            "Epoch 13/200\n",
            "2643/2643 - 31s - loss: 0.1449 - val_loss: 0.1442 - lr: 0.0010 - 31s/epoch - 12ms/step\n",
            "Epoch 14/200\n",
            "2643/2643 - 31s - loss: 0.1447 - val_loss: 0.1444 - lr: 0.0010 - 31s/epoch - 12ms/step\n",
            "Epoch 15/200\n",
            "2643/2643 - 31s - loss: 0.1446 - val_loss: 0.1440 - lr: 0.0010 - 31s/epoch - 12ms/step\n",
            "Epoch 16/200\n",
            "2643/2643 - 31s - loss: 0.1445 - val_loss: 0.1452 - lr: 0.0010 - 31s/epoch - 12ms/step\n",
            "Epoch 17/200\n",
            "2643/2643 - 31s - loss: 0.1444 - val_loss: 0.1443 - lr: 0.0010 - 31s/epoch - 12ms/step\n",
            "Epoch 18/200\n",
            "2643/2643 - 31s - loss: 0.1443 - val_loss: 0.1432 - lr: 0.0010 - 31s/epoch - 12ms/step\n",
            "Epoch 19/200\n",
            "2643/2643 - 31s - loss: 0.1442 - val_loss: 0.1435 - lr: 0.0010 - 31s/epoch - 12ms/step\n",
            "Epoch 20/200\n",
            "2643/2643 - 31s - loss: 0.1434 - val_loss: 0.1428 - lr: 5.0000e-04 - 31s/epoch - 12ms/step\n",
            "Epoch 21/200\n",
            "2643/2643 - 31s - loss: 0.1434 - val_loss: 0.1431 - lr: 5.0000e-04 - 31s/epoch - 12ms/step\n",
            "Epoch 22/200\n",
            "2643/2643 - 31s - loss: 0.1432 - val_loss: 0.1428 - lr: 5.0000e-04 - 31s/epoch - 12ms/step\n",
            "Epoch 23/200\n",
            "2643/2643 - 30s - loss: 0.1432 - val_loss: 0.1428 - lr: 5.0000e-04 - 30s/epoch - 12ms/step\n",
            "Epoch 24/200\n",
            "2643/2643 - 31s - loss: 0.1432 - val_loss: 0.1431 - lr: 5.0000e-04 - 31s/epoch - 12ms/step\n",
            "Epoch 25/200\n",
            "2643/2643 - 31s - loss: 0.1431 - val_loss: 0.1424 - lr: 5.0000e-04 - 31s/epoch - 12ms/step\n",
            "Epoch 26/200\n",
            "2643/2643 - 30s - loss: 0.1431 - val_loss: 0.1423 - lr: 5.0000e-04 - 30s/epoch - 12ms/step\n",
            "Epoch 27/200\n",
            "2643/2643 - 31s - loss: 0.1430 - val_loss: 0.1428 - lr: 5.0000e-04 - 31s/epoch - 12ms/step\n",
            "Epoch 28/200\n",
            "2643/2643 - 31s - loss: 0.1430 - val_loss: 0.1429 - lr: 5.0000e-04 - 31s/epoch - 12ms/step\n",
            "Epoch 29/200\n",
            "2643/2643 - 31s - loss: 0.1429 - val_loss: 0.1434 - lr: 5.0000e-04 - 31s/epoch - 12ms/step\n",
            "Epoch 30/200\n",
            "2643/2643 - 31s - loss: 0.1429 - val_loss: 0.1425 - lr: 5.0000e-04 - 31s/epoch - 12ms/step\n",
            "Epoch 31/200\n",
            "2643/2643 - 31s - loss: 0.1428 - val_loss: 0.1425 - lr: 5.0000e-04 - 31s/epoch - 12ms/step\n",
            "Epoch 32/200\n",
            "2643/2643 - 31s - loss: 0.1428 - val_loss: 0.1429 - lr: 5.0000e-04 - 31s/epoch - 12ms/step\n",
            "Epoch 33/200\n",
            "2643/2643 - 31s - loss: 0.1428 - val_loss: 0.1434 - lr: 5.0000e-04 - 31s/epoch - 12ms/step\n",
            "Epoch 34/200\n",
            "2643/2643 - 31s - loss: 0.1428 - val_loss: 0.1424 - lr: 5.0000e-04 - 31s/epoch - 12ms/step\n",
            "Epoch 35/200\n",
            "2643/2643 - 31s - loss: 0.1427 - val_loss: 0.1432 - lr: 5.0000e-04 - 31s/epoch - 12ms/step\n",
            "Epoch 36/200\n",
            "2643/2643 - 31s - loss: 0.1427 - val_loss: 0.1429 - lr: 5.0000e-04 - 31s/epoch - 12ms/step\n",
            "Epoch 37/200\n",
            "2643/2643 - 31s - loss: 0.1427 - val_loss: 0.1424 - lr: 5.0000e-04 - 31s/epoch - 12ms/step\n",
            "Epoch 38/200\n",
            "2643/2643 - 32s - loss: 0.1427 - val_loss: 0.1422 - lr: 5.0000e-04 - 32s/epoch - 12ms/step\n",
            "Epoch 39/200\n",
            "2643/2643 - 31s - loss: 0.1426 - val_loss: 0.1428 - lr: 5.0000e-04 - 31s/epoch - 12ms/step\n",
            "Epoch 40/200\n",
            "2643/2643 - 31s - loss: 0.1421 - val_loss: 0.1424 - lr: 2.5000e-04 - 31s/epoch - 12ms/step\n",
            "Epoch 41/200\n",
            "2643/2643 - 31s - loss: 0.1421 - val_loss: 0.1426 - lr: 2.5000e-04 - 31s/epoch - 12ms/step\n",
            "Epoch 42/200\n",
            "2643/2643 - 32s - loss: 0.1420 - val_loss: 0.1424 - lr: 2.5000e-04 - 32s/epoch - 12ms/step\n",
            "Epoch 43/200\n",
            "2643/2643 - 31s - loss: 0.1420 - val_loss: 0.1423 - lr: 2.5000e-04 - 31s/epoch - 12ms/step\n",
            "Epoch 44/200\n",
            "2643/2643 - 31s - loss: 0.1420 - val_loss: 0.1420 - lr: 2.5000e-04 - 31s/epoch - 12ms/step\n",
            "Epoch 45/200\n",
            "2643/2643 - 31s - loss: 0.1419 - val_loss: 0.1419 - lr: 2.5000e-04 - 31s/epoch - 12ms/step\n",
            "Epoch 46/200\n",
            "2643/2643 - 31s - loss: 0.1419 - val_loss: 0.1421 - lr: 2.5000e-04 - 31s/epoch - 12ms/step\n",
            "Epoch 47/200\n",
            "2643/2643 - 31s - loss: 0.1419 - val_loss: 0.1424 - lr: 2.5000e-04 - 31s/epoch - 12ms/step\n",
            "Epoch 48/200\n",
            "2643/2643 - 31s - loss: 0.1419 - val_loss: 0.1425 - lr: 2.5000e-04 - 31s/epoch - 12ms/step\n",
            "Epoch 49/200\n",
            "2643/2643 - 31s - loss: 0.1419 - val_loss: 0.1423 - lr: 2.5000e-04 - 31s/epoch - 12ms/step\n",
            "Epoch 50/200\n",
            "2643/2643 - 31s - loss: 0.1418 - val_loss: 0.1423 - lr: 2.5000e-04 - 31s/epoch - 12ms/step\n",
            "Epoch 51/200\n",
            "2643/2643 - 31s - loss: 0.1418 - val_loss: 0.1420 - lr: 2.5000e-04 - 31s/epoch - 12ms/step\n",
            "Epoch 52/200\n",
            "2643/2643 - 31s - loss: 0.1418 - val_loss: 0.1420 - lr: 2.5000e-04 - 31s/epoch - 12ms/step\n",
            "Epoch 53/200\n",
            "2643/2643 - 31s - loss: 0.1418 - val_loss: 0.1421 - lr: 2.5000e-04 - 31s/epoch - 12ms/step\n",
            "Epoch 54/200\n",
            "2643/2643 - 31s - loss: 0.1418 - val_loss: 0.1421 - lr: 2.5000e-04 - 31s/epoch - 12ms/step\n",
            "Epoch 55/200\n",
            "2643/2643 - 31s - loss: 0.1417 - val_loss: 0.1421 - lr: 2.5000e-04 - 31s/epoch - 12ms/step\n",
            "Epoch 56/200\n",
            "2643/2643 - 31s - loss: 0.1417 - val_loss: 0.1418 - lr: 2.5000e-04 - 31s/epoch - 12ms/step\n",
            "Epoch 57/200\n",
            "2643/2643 - 31s - loss: 0.1417 - val_loss: 0.1418 - lr: 2.5000e-04 - 31s/epoch - 12ms/step\n",
            "Epoch 58/200\n",
            "2643/2643 - 31s - loss: 0.1417 - val_loss: 0.1421 - lr: 2.5000e-04 - 31s/epoch - 12ms/step\n",
            "Epoch 59/200\n",
            "2643/2643 - 31s - loss: 0.1417 - val_loss: 0.1425 - lr: 2.5000e-04 - 31s/epoch - 12ms/step\n",
            "Epoch 60/200\n",
            "2643/2643 - 31s - loss: 0.1414 - val_loss: 0.1420 - lr: 1.2500e-04 - 31s/epoch - 12ms/step\n",
            "Epoch 61/200\n",
            "2643/2643 - 31s - loss: 0.1413 - val_loss: 0.1419 - lr: 1.2500e-04 - 31s/epoch - 12ms/step\n",
            "Epoch 62/200\n",
            "2643/2643 - 31s - loss: 0.1413 - val_loss: 0.1421 - lr: 1.2500e-04 - 31s/epoch - 12ms/step\n",
            "Epoch 63/200\n",
            "2643/2643 - 31s - loss: 0.1413 - val_loss: 0.1421 - lr: 1.2500e-04 - 31s/epoch - 12ms/step\n",
            "Epoch 64/200\n",
            "2643/2643 - 31s - loss: 0.1413 - val_loss: 0.1419 - lr: 1.2500e-04 - 31s/epoch - 12ms/step\n",
            "Epoch 65/200\n",
            "2643/2643 - 31s - loss: 0.1413 - val_loss: 0.1417 - lr: 1.2500e-04 - 31s/epoch - 12ms/step\n",
            "Epoch 66/200\n",
            "2643/2643 - 31s - loss: 0.1413 - val_loss: 0.1421 - lr: 1.2500e-04 - 31s/epoch - 12ms/step\n",
            "Epoch 67/200\n",
            "2643/2643 - 31s - loss: 0.1412 - val_loss: 0.1420 - lr: 1.2500e-04 - 31s/epoch - 12ms/step\n",
            "Epoch 68/200\n",
            "2643/2643 - 31s - loss: 0.1412 - val_loss: 0.1418 - lr: 1.2500e-04 - 31s/epoch - 12ms/step\n",
            "Epoch 69/200\n",
            "2643/2643 - 31s - loss: 0.1412 - val_loss: 0.1419 - lr: 1.2500e-04 - 31s/epoch - 12ms/step\n",
            "Epoch 70/200\n",
            "2643/2643 - 31s - loss: 0.1412 - val_loss: 0.1419 - lr: 1.2500e-04 - 31s/epoch - 12ms/step\n",
            "Epoch 71/200\n",
            "2643/2643 - 32s - loss: 0.1412 - val_loss: 0.1420 - lr: 1.2500e-04 - 32s/epoch - 12ms/step\n",
            "Epoch 72/200\n",
            "2643/2643 - 31s - loss: 0.1412 - val_loss: 0.1422 - lr: 1.2500e-04 - 31s/epoch - 12ms/step\n",
            "Epoch 73/200\n",
            "2643/2643 - 31s - loss: 0.1412 - val_loss: 0.1424 - lr: 1.2500e-04 - 31s/epoch - 12ms/step\n",
            "Epoch 74/200\n",
            "2643/2643 - 32s - loss: 0.1411 - val_loss: 0.1422 - lr: 1.2500e-04 - 32s/epoch - 12ms/step\n",
            "Epoch 75/200\n",
            "2643/2643 - 32s - loss: 0.1411 - val_loss: 0.1420 - lr: 1.2500e-04 - 32s/epoch - 12ms/step\n",
            "Epoch 76/200\n",
            "2643/2643 - 31s - loss: 0.1411 - val_loss: 0.1418 - lr: 1.2500e-04 - 31s/epoch - 12ms/step\n",
            "Epoch 77/200\n",
            "2643/2643 - 31s - loss: 0.1411 - val_loss: 0.1417 - lr: 1.2500e-04 - 31s/epoch - 12ms/step\n",
            "Epoch 78/200\n",
            "2643/2643 - 31s - loss: 0.1411 - val_loss: 0.1418 - lr: 1.2500e-04 - 31s/epoch - 12ms/step\n",
            "Epoch 79/200\n",
            "2643/2643 - 31s - loss: 0.1411 - val_loss: 0.1420 - lr: 1.2500e-04 - 31s/epoch - 12ms/step\n",
            "Epoch 80/200\n",
            "2643/2643 - 31s - loss: 0.1409 - val_loss: 0.1417 - lr: 6.2500e-05 - 31s/epoch - 12ms/step\n",
            "Epoch 81/200\n",
            "2643/2643 - 32s - loss: 0.1409 - val_loss: 0.1419 - lr: 6.2500e-05 - 32s/epoch - 12ms/step\n",
            "Epoch 82/200\n",
            "2643/2643 - 31s - loss: 0.1409 - val_loss: 0.1420 - lr: 6.2500e-05 - 31s/epoch - 12ms/step\n",
            "Epoch 83/200\n",
            "2643/2643 - 32s - loss: 0.1409 - val_loss: 0.1420 - lr: 6.2500e-05 - 32s/epoch - 12ms/step\n",
            "Epoch 84/200\n",
            "2643/2643 - 32s - loss: 0.1409 - val_loss: 0.1419 - lr: 6.2500e-05 - 32s/epoch - 12ms/step\n",
            "Epoch 85/200\n",
            "2643/2643 - 31s - loss: 0.1408 - val_loss: 0.1422 - lr: 6.2500e-05 - 31s/epoch - 12ms/step\n",
            "Epoch 86/200\n",
            "2643/2643 - 32s - loss: 0.1408 - val_loss: 0.1420 - lr: 6.2500e-05 - 32s/epoch - 12ms/step\n",
            "Epoch 87/200\n",
            "2643/2643 - 32s - loss: 0.1408 - val_loss: 0.1419 - lr: 6.2500e-05 - 32s/epoch - 12ms/step\n",
            "Epoch 88/200\n",
            "2643/2643 - 32s - loss: 0.1408 - val_loss: 0.1418 - lr: 6.2500e-05 - 32s/epoch - 12ms/step\n",
            "Epoch 89/200\n",
            "2643/2643 - 31s - loss: 0.1408 - val_loss: 0.1420 - lr: 6.2500e-05 - 31s/epoch - 12ms/step\n",
            "Epoch 90/200\n",
            "2643/2643 - 32s - loss: 0.1408 - val_loss: 0.1419 - lr: 6.2500e-05 - 32s/epoch - 12ms/step\n",
            "Epoch 91/200\n",
            "2643/2643 - 31s - loss: 0.1408 - val_loss: 0.1420 - lr: 6.2500e-05 - 31s/epoch - 12ms/step\n",
            "Epoch 92/200\n",
            "2643/2643 - 31s - loss: 0.1408 - val_loss: 0.1419 - lr: 6.2500e-05 - 31s/epoch - 12ms/step\n",
            "Epoch 93/200\n",
            "2643/2643 - 31s - loss: 0.1408 - val_loss: 0.1418 - lr: 6.2500e-05 - 31s/epoch - 12ms/step\n",
            "Epoch 94/200\n",
            "2643/2643 - 31s - loss: 0.1408 - val_loss: 0.1420 - lr: 6.2500e-05 - 31s/epoch - 12ms/step\n",
            "Epoch 95/200\n",
            "2643/2643 - 31s - loss: 0.1408 - val_loss: 0.1422 - lr: 6.2500e-05 - 31s/epoch - 12ms/step\n",
            "Epoch 96/200\n",
            "2643/2643 - 31s - loss: 0.1408 - val_loss: 0.1419 - lr: 6.2500e-05 - 31s/epoch - 12ms/step\n",
            "Epoch 97/200\n",
            "2643/2643 - 31s - loss: 0.1407 - val_loss: 0.1419 - lr: 6.2500e-05 - 31s/epoch - 12ms/step\n",
            "Epoch 98/200\n",
            "2643/2643 - 31s - loss: 0.1407 - val_loss: 0.1420 - lr: 6.2500e-05 - 31s/epoch - 12ms/step\n",
            "Epoch 99/200\n",
            "2643/2643 - 31s - loss: 0.1407 - val_loss: 0.1419 - lr: 6.2500e-05 - 31s/epoch - 12ms/step\n",
            "Epoch 100/200\n",
            "2643/2643 - 31s - loss: 0.1406 - val_loss: 0.1420 - lr: 3.1250e-05 - 31s/epoch - 12ms/step\n",
            "Epoch 101/200\n",
            "2643/2643 - 31s - loss: 0.1406 - val_loss: 0.1420 - lr: 3.1250e-05 - 31s/epoch - 12ms/step\n",
            "Epoch 102/200\n",
            "2643/2643 - 31s - loss: 0.1406 - val_loss: 0.1420 - lr: 3.1250e-05 - 31s/epoch - 12ms/step\n",
            "Epoch 103/200\n",
            "2643/2643 - 32s - loss: 0.1406 - val_loss: 0.1419 - lr: 3.1250e-05 - 32s/epoch - 12ms/step\n",
            "Epoch 104/200\n",
            "2643/2643 - 31s - loss: 0.1406 - val_loss: 0.1419 - lr: 3.1250e-05 - 31s/epoch - 12ms/step\n",
            "Epoch 105/200\n",
            "2643/2643 - 32s - loss: 0.1406 - val_loss: 0.1419 - lr: 3.1250e-05 - 32s/epoch - 12ms/step\n",
            "Epoch 106/200\n",
            "2643/2643 - 31s - loss: 0.1406 - val_loss: 0.1421 - lr: 3.1250e-05 - 31s/epoch - 12ms/step\n",
            "Epoch 107/200\n",
            "2643/2643 - 31s - loss: 0.1406 - val_loss: 0.1420 - lr: 3.1250e-05 - 31s/epoch - 12ms/step\n",
            "Epoch 108/200\n",
            "2643/2643 - 31s - loss: 0.1406 - val_loss: 0.1420 - lr: 3.1250e-05 - 31s/epoch - 12ms/step\n",
            "Epoch 109/200\n",
            "2643/2643 - 32s - loss: 0.1406 - val_loss: 0.1421 - lr: 3.1250e-05 - 32s/epoch - 12ms/step\n",
            "Epoch 110/200\n",
            "2643/2643 - 31s - loss: 0.1406 - val_loss: 0.1420 - lr: 3.1250e-05 - 31s/epoch - 12ms/step\n",
            "Epoch 110: early stopping\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-fbd7d9062525>:12: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  save_model(model, str(model_save_path))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved\n",
            "3860.5747726  s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save_model(model, str(model_save_path))\n"
      ],
      "metadata": {
        "id": "QHqvt4OT2CW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tf.keras.backend.clear_session()\n",
        "# print(\"End of code\")"
      ],
      "metadata": {
        "id": "1C-kYOGV2KGX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}